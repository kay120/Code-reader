"""
WebVectorizeRepoNode - Webå‘é‡åŒ–èŠ‚ç‚¹ï¼Œä»åç«¯APIè·å–æ–‡ä»¶å†…å®¹å¹¶åˆ›å»ºå‘é‡çŸ¥è¯†åº“
Design: AsyncNode, max_retries=2, wait=30
"""

import aiohttp
import asyncio
import logging
from typing import Dict, Any, Tuple
from pathlib import Path
from pocketflow import AsyncNode

from ..utils.config import get_config

# è®¾ç½®logger
logger = logging.getLogger(__name__)


class WebVectorizeRepoNode(AsyncNode):
    """Webå‘é‡åŒ–èŠ‚ç‚¹ - ä»åç«¯APIè·å–æ–‡ä»¶å†…å®¹å¹¶åˆ›å»ºå‘é‡çŸ¥è¯†åº“"""

    def __init__(self):
        super().__init__(max_retries=2, wait=30)
        self.config = get_config()
        self.api_base_url = self.config.api_base_url
        self.rag_base_url = self.config.rag_base_url
        self.rag_batch_size = self.config.rag_batch_size

    async def prep_async(self, shared: Dict[str, Any]) -> Tuple[int, Dict[str, Any]]:
        """
        å‡†å¤‡å‘é‡åŒ–æ“ä½œ - è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨

        Data Access:
        - Read: shared.task_id (ä»»åŠ¡ID)
        - Read: shared.repo_info (ä»“åº“ä¿¡æ¯)
        """
        logger.info("=" * 60)
        logger.info("ğŸ“‹ é˜¶æ®µ: Webå‘é‡åŒ–æ„å»º (WebVectorizeRepoNode)")
        shared["current_stage"] = "vectorization"

        task_id = shared.get("task_id")
        repo_info = shared.get("repo_info")

        if not task_id:
            logger.error("âŒ å‘é‡åŒ–æ„å»ºéœ€è¦æä¾›ä»»åŠ¡ID")
            raise ValueError("Task ID is required for vectorization")

        if not repo_info:
            logger.error("âŒ å‘é‡åŒ–æ„å»ºéœ€è¦æä¾›ä»“åº“ä¿¡æ¯")
            raise ValueError("Repository info is required for vectorization")

        logger.info(f"ğŸ” å‡†å¤‡ä¸ºä»»åŠ¡ {task_id} æ„å»ºå‘é‡çŸ¥è¯†åº“")
        return task_id, repo_info

    async def exec_async(self, prep_res: Tuple[int, Dict[str, Any]]) -> str:
        """
        æ‰§è¡Œå‘é‡åŒ–æ“ä½œ - ä»APIè·å–æ–‡ä»¶å†…å®¹å¹¶åˆ›å»ºå‘é‡çŸ¥è¯†åº“
        """
        task_id, repo_info = prep_res

        # 1. ä»åç«¯APIè·å–æ–‡ä»¶å†…å®¹
        logger.info(f"ğŸ“¥ ä»APIè·å–ä»»åŠ¡ {task_id} çš„æ–‡ä»¶å†…å®¹...")
        await asyncio.sleep(1)  # 1ç§’å»¶è¿Ÿè®©ç”¨æˆ·çœ‹åˆ°å¼€å§‹çŠ¶æ€

        documents = await self._fetch_documents_from_api(task_id)

        if not documents:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯å‘é‡åŒ–çš„æ–‡æ¡£,ä½¿ç”¨ç©ºç´¢å¼•")
            store_id = self._generate_store_id(repo_info)
            return f"local_{store_id}_empty"

        logger.info(f"ğŸ“„ è·å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        await asyncio.sleep(1)  # 1ç§’å»¶è¿Ÿ

        # 2. åˆ›å»ºå‘é‡çŸ¥è¯†åº“
        store_id = self._generate_store_id(repo_info)
        logger.info(f"ğŸš€ å¼€å§‹ä¸ºä»“åº“ {store_id} åˆ›å»ºå‘é‡çŸ¥è¯†åº“")
        await asyncio.sleep(2)  # 2ç§’å»¶è¿Ÿè®©ç”¨æˆ·çœ‹åˆ°åˆ›å»ºè¿‡ç¨‹

        index_name = await self._create_vector_store(documents, store_id)

        if not index_name:
            logger.warning("âš ï¸ å‘é‡çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥,ä½¿ç”¨æœ¬åœ°ç´¢å¼•")
            return f"local_{store_id}"

        logger.info(f"âœ… å‘é‡çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•: {index_name}")
        await asyncio.sleep(1)  # 1ç§’å»¶è¿Ÿè®©ç”¨æˆ·çœ‹åˆ°å®ŒæˆçŠ¶æ€
        return index_name

    async def post_async(self, shared: Dict[str, Any], prep_res: Tuple, exec_res: str) -> str:
        """
        å‘é‡åŒ–åå¤„ç†

        Data Access:
        - Write: shared.vectorstore_index (RAG API ç´¢å¼•åç§°)
        """
        shared["vectorstore_index"] = exec_res
        logger.info(f"ğŸ“‚ å‘é‡ç´¢å¼•å·²è®¾ç½®: {exec_res}")
        return "default"

    async def _fetch_documents_from_api(self, task_id: int) -> list:
        """ä»åç«¯APIè·å–æ–‡ä»¶å†…å®¹"""
        try:
            api_url = f"{self.api_base_url}/api/repository/files/{task_id}?include_code_content=true"

            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, timeout=aiohttp.ClientTimeout(total=60)) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get("status") == "success" and result.get("files"):
                            return self._convert_files_to_documents(result["files"])
                        else:
                            print(result)
                            logger.error(f"APIè¿”å›é”™è¯¯: {result.get('message', 'Unknown error')}")
                            return []
                    else:
                        error_text = await response.text()
                        print(response)
                        logger.error(f"APIè¯·æ±‚å¤±è´¥: HTTP {response.status} - {error_text}")
                        return []
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
            return []

    def _convert_files_to_documents(self, files: list) -> list:
        """å°†æ–‡ä»¶è®°å½•è½¬æ¢ä¸ºRAGæ–‡æ¡£æ ¼å¼"""
        documents = []

        for file_record in files:
            try:
                # è·³è¿‡æ²¡æœ‰å†…å®¹çš„æ–‡ä»¶
                if not file_record.get("code_content") or file_record["code_content"].strip() == "":
                    continue

                # æ ¹æ®æ–‡ä»¶ç±»å‹ç¡®å®šç±»åˆ«
                file_path = file_record.get("file_path", "")
                doc_exts = [".md", ".mdx", ".rst", ".txt", ".adoc","py"]
                file_extension = "." + file_path.split(".")[-1].lower() if "." in file_path else ""
                category = "æ–‡æ¡£" if file_extension in doc_exts else "ä»£ç "

                # æ„å»ºæ–‡æ¡£å¯¹è±¡
                document = {
                    "title": file_path.split("/")[-1] if "/" in file_path else file_path,
                    "file": file_path,
                    "content": file_record["code_content"],
                    "category": category,
                    "language": file_record.get("language", "text"),
                    "start_line": 1,
                    "end_line": file_record.get("code_lines", 1),
                }

                documents.append(document)

            except Exception as e:
                logger.warning(f"è½¬æ¢æ–‡ä»¶è®°å½•å¤±è´¥: {str(e)}")
                continue

        return documents

    def _generate_store_id(self, repo_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆå‘é‡å­˜å‚¨çš„å”¯ä¸€ID"""
        full_name = repo_info.get("full_name", "unknown")
        if "/" in full_name:
            repo_name = full_name.split("/")[-1]
        else:
            repo_name = full_name
        return repo_name

    async def _create_vector_store(self, documents: list, store_id: str) -> str:
        """åˆ›å»ºå‘é‡çŸ¥è¯†åº“"""
        try:
            # æ£€æŸ¥RAGæœåŠ¡å¥åº·çŠ¶æ€
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.rag_base_url}/health", timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status != 200:
                        raise ValueError("RAG API æœåŠ¡ä¸å¯ç”¨")

            logger.info("âœ… RAG API æœåŠ¡è¿è¡Œæ­£å¸¸")

            # åˆ†æ‰¹å¤„ç†æ–‡æ¡£
            batch_size = self.rag_batch_size
            index_name = None

            if batch_size <= 0 or batch_size >= len(documents):
                # ä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰æ–‡æ¡£
                logger.info(f"ä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰æ–‡æ¡£ï¼ˆå…± {len(documents)} æ¡ï¼‰")
                index_name = await self._create_knowledge_base(documents, store_id)
            else:
                # åˆ†æ‰¹ä¸Šä¼ 
                for i in range(0, len(documents), batch_size):
                    batch = documents[i : i + batch_size]
                    batch_num = i // batch_size + 1
                    total_batches = (len(documents) + batch_size - 1) // batch_size

                    logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ–‡æ¡£ ({len(batch)} ä¸ªæ–‡æ¡£)")

                    if i == 0:
                        # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ–°çš„çŸ¥è¯†åº“
                        index_name = await self._create_knowledge_base(batch, store_id)
                        if not index_name:
                            raise ValueError("åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥")
                    else:
                        # åç»­æ‰¹æ¬¡ï¼šæ·»åŠ åˆ°å·²å­˜åœ¨çš„çŸ¥è¯†åº“
                        success = await self._add_documents_to_index(batch, index_name)
                        if not success:
                            logger.warning(f"ç¬¬ {batch_num} æ‰¹æ–‡æ¡£æ·»åŠ å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹")

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
                    if i + batch_size < len(documents):
                        await asyncio.sleep(1)

            return index_name

        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            raise

    async def _create_knowledge_base(self, documents: list, store_id: str) -> str:
        """è°ƒç”¨RAG APIåˆ›å»ºçŸ¥è¯†åº“"""
        try:
            request_data = {"documents": documents, "vector_field": "content"}

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.rag_base_url}/documents",
                    json=request_data,
                    headers={"Content-Type": "application/json"},
                    timeout=aiohttp.ClientTimeout(total=300),
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        index_name = result["index"]
                        count = result["count"]
                        logger.info(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•: {index_name}, æ–‡æ¡£æ•°é‡: {count}")
                        return index_name
                    else:
                        error_text = await response.text()
                        logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: HTTP {response.status} - {error_text}")
                        return None
        except Exception as e:
            logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            # RAG æœåŠ¡ä¸å¯ç”¨æ—¶,è¿”å›ä¸€ä¸ªæœ¬åœ°ç´¢å¼•åç§°,å…è®¸åç»­æ­¥éª¤ç»§ç»­æ‰§è¡Œ
            logger.warning(f"âš ï¸ RAG æœåŠ¡ä¸å¯ç”¨,ä½¿ç”¨æœ¬åœ°ç´¢å¼•åç§°: {store_id}")
            logger.warning(f"âš ï¸ æ³¨æ„: ä»£ç åˆ†æå°†åœ¨æ²¡æœ‰å‘é‡æ£€ç´¢ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹è¿›è¡Œ")
            return f"local_{store_id}"

    async def _add_documents_to_index(self, documents: list, index_name: str) -> bool:
        """å‘å·²å­˜åœ¨çš„ç´¢å¼•æ·»åŠ æ–‡æ¡£"""
        try:
            request_data = {"documents": documents, "vector_field": "content", "index": index_name}

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.rag_base_url}/documents",
                    json=request_data,
                    headers={"Content-Type": "application/json"},
                    timeout=aiohttp.ClientTimeout(total=300),
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        count = result["count"]
                        logger.info(f"âœ… æˆåŠŸæ·»åŠ  {count} ä¸ªæ–‡æ¡£åˆ°ç´¢å¼•")
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: HTTP {response.status} - {error_text}")
                        return False
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            return False

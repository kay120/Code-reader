"""
Web çŸ¥è¯†åº“åˆ›å»ºæµç¨‹
ç”¨äºå¤„ç†å‰ç«¯ä¸Šä¼ æ–‡ä»¶åçš„çŸ¥è¯†åº“åˆ›å»ºæ“ä½œ
"""

import asyncio
import logging
from typing import Dict, Any, List
from pathlib import Path
from pocketflow import AsyncFlow

from ..nodes.web_vectorize_repo_node import WebVectorizeRepoNode
from ..nodes.rag_database_update_node import RAGDatabaseUpdateNode

from ..utils.llm_parser import LLMParser
from ..utils.config import get_config

# è®¾ç½®logger
logger = logging.getLogger(__name__)


class WebKnowledgeBaseFlow(AsyncFlow):
    """Web çŸ¥è¯†åº“åˆ›å»ºæµç¨‹"""

    def __init__(self):
        super().__init__()

        # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
        self.vectorize_node = WebVectorizeRepoNode()
        self.database_update_node = RAGDatabaseUpdateNode()

        # æ„å»ºæµç¨‹é“¾
        self._build_flow()

    def _build_flow(self):
        """æ„å»ºçŸ¥è¯†åº“åˆ›å»ºæµç¨‹"""
        # è®¾ç½®èµ·å§‹èŠ‚ç‚¹
        self.start(self.vectorize_node)

        # æ„å»ºèŠ‚ç‚¹é“¾ï¼šå‘é‡åŒ– -> æ•°æ®åº“æ›´æ–°
        self.vectorize_node >> self.database_update_node

        logger.info("Web knowledge base flow constructed")

    async def prep_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """æµç¨‹é¢„å¤„ç†"""
        logger.info("ğŸš€ ========== å¼€å§‹ Web çŸ¥è¯†åº“åˆ›å»ºæµç¨‹ ==========")
        logger.info("ğŸ“‹ é˜¶æ®µ: æµç¨‹åˆå§‹åŒ– (WebKnowledgeBaseFlow.prep_async)")

        # éªŒè¯å¿…éœ€çš„è¾“å…¥å‚æ•°
        required_fields = ["task_id", "local_path", "repo_info"]
        for field in required_fields:
            if field not in shared:
                logger.error(f"âŒ ç¼ºå°‘å¿…éœ€å‚æ•°: {field}")
                raise ValueError(f"Required field '{field}' is missing from shared data")

        task_id = shared.get("task_id")
        local_path = shared.get("local_path")
        repo_info = shared.get("repo_info")

        # éªŒè¯å‚æ•°ç±»å‹å’Œå€¼
        if not isinstance(task_id, int) or task_id <= 0:
            logger.error(f"âŒ ä»»åŠ¡IDæ— æ•ˆ: {task_id}")
            raise ValueError("Task ID must be a positive integer")

        if not local_path or not isinstance(local_path, (str, Path)):
            logger.error(f"âŒ æœ¬åœ°è·¯å¾„æ— æ•ˆ: {local_path}")
            raise ValueError("Local path must be a valid string or Path object")

        if not repo_info or not isinstance(repo_info, dict):
            logger.error(f"âŒ ä»“åº“ä¿¡æ¯æ— æ•ˆ: {repo_info}")
            raise ValueError("Repository info must be a valid dictionary")

        # æ³¨æ„ï¼šWebVectorizeRepoNodeä»APIè·å–æ•°æ®ï¼Œä¸éœ€è¦éªŒè¯æœ¬åœ°è·¯å¾„
        # è¿™é‡Œä¿ç•™local_pathå‚æ•°æ˜¯ä¸ºäº†å…¼å®¹æ€§ï¼Œä½†å®é™…ä¸ä½¿ç”¨

        logger.info(f"ğŸ¯ ä»»åŠ¡ID: {task_id}")
        logger.info(f"ğŸ“ æœ¬åœ°è·¯å¾„: {local_path}")
        logger.info(f"ğŸ“Š ä»“åº“ä¿¡æ¯: {repo_info.get('full_name', 'Unknown')}")

        # åˆå§‹åŒ–å…±äº«çŠ¶æ€
        shared.setdefault("status", "processing")
        shared["current_stage"] = "initialization"

        # æ·»åŠ è¿›åº¦æ˜¾ç¤ºå»¶è¿Ÿï¼Œè®©ç”¨æˆ·çœ‹åˆ°åˆå§‹åŒ–è¿‡ç¨‹
        logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“åˆ›å»ºæµç¨‹...")
        await asyncio.sleep(2)  # 2ç§’å»¶è¿Ÿ

        logger.info("âœ… æµç¨‹åˆå§‹åŒ–å®Œæˆ")
        return shared

    async def post_async(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: str) -> Dict[str, Any]:
        """æµç¨‹åå¤„ç†"""
        logger.info("ğŸ“‹ é˜¶æ®µ: æµç¨‹åå¤„ç† (WebKnowledgeBaseFlow.post_async)")

        # prep_res å’Œ exec_res æ˜¯ pocketflow æ¡†æ¶ä¼ é€’çš„å‚æ•°ï¼Œè¿™é‡Œä¸éœ€è¦ä½¿ç”¨
        # æˆ‘ä»¬é€šè¿‡ shared çŠ¶æ€æ¥åˆ¤æ–­æµç¨‹æ‰§è¡Œç»“æœ
        _ = prep_res, exec_res  # é¿å…æœªä½¿ç”¨å‚æ•°è­¦å‘Š

        # æ£€æŸ¥æµç¨‹æ‰§è¡Œç»“æœ
        if shared.get("vectorstore_index") and shared.get("database_updated"):
            shared["status"] = "completed"
            logger.info(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæµç¨‹å®Œæˆ")
            logger.info(f"ğŸ“‚ å‘é‡ç´¢å¼•: {shared.get('vectorstore_index')}")
        else:
            shared["status"] = "failed"
            logger.error("âŒ çŸ¥è¯†åº“åˆ›å»ºæµç¨‹å¤±è´¥")

        logger.info("ğŸ ========== Web çŸ¥è¯†åº“åˆ›å»ºæµç¨‹ç»“æŸ ==========")
        return shared


async def create_knowledge_base(
    task_id: int, local_path: str, repo_info: Dict[str, Any], progress_callback=None
) -> Dict[str, Any]:
    """
    åˆ›å»ºçŸ¥è¯†åº“çš„ä¾¿æ·å‡½æ•°

    Args:
        task_id: ä»»åŠ¡ID
        local_path: æœ¬åœ°ä»“åº“è·¯å¾„
        repo_info: ä»“åº“ä¿¡æ¯å­—å…¸
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        åˆ›å»ºç»“æœå­—å…¸
    """
    # å‡†å¤‡å…±äº«æ•°æ®
    shared = {"task_id": task_id, "local_path": local_path, "repo_info": repo_info, "status": "processing"}

    if progress_callback:
        shared["progress_callback"] = progress_callback

    # åˆ›å»ºå¹¶æ‰§è¡Œæµç¨‹
    flow = WebKnowledgeBaseFlow()

    try:
        # æ‰§è¡ŒçŸ¥è¯†åº“åˆ›å»ºæµç¨‹
        await flow.run_async(shared)

        # çŸ¥è¯†åº“åˆ›å»ºå®Œæˆåï¼Œå°†ç»“æœä¿å­˜åˆ°æ•°æ®åº“
        if shared.get("vectorstore_index") and shared.get("database_updated"):
            logger.info(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•: {shared.get('vectorstore_index')}")
            # æ³¨æ„ï¼štask_indexå·²ç»é€šè¿‡RAGDatabaseUpdateNodeæ›´æ–°åˆ°æ•°æ®åº“äº†
            # è¿™é‡Œä¸éœ€è¦å†æ¬¡æ›´æ–°æ•°æ®åº“
            shared["status"] = "knowledge_base_ready"  # ä½¿ç”¨ä¸­é—´çŠ¶æ€
        else:
            logger.error("âŒ çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥ï¼šç¼ºå°‘å‘é‡ç´¢å¼•æˆ–æ•°æ®åº“æ›´æ–°å¤±è´¥")
            shared["status"] = "failed"
            shared["error"] = "Knowledge base creation incomplete"

        # è¿”å›å®Œæ•´çš„å…±äº«æ•°æ®
        return shared

    except Exception as e:
        logger.error(f"Knowledge base creation failed for task {task_id}: {str(e)}")
        shared["status"] = "failed"
        shared["error"] = str(e)
        return shared


class WebAnalysisFlow(AsyncFlow):
    """Web å•æ–‡ä»¶åˆ†ææµç¨‹"""

    def __init__(self):
        super().__init__()

        # å¯¼å…¥å¿…è¦çš„æ¨¡å—

        # åˆå§‹åŒ–LLMè§£æå™¨ï¼ˆè‡ªåŠ¨ä»é…ç½®è·å–å‚æ•°ï¼‰
        self.llm_parser = LLMParser()

        # è·å–APIåŸºç¡€URL
        config = get_config()
        self.api_base_url = config.api_base_url

    async def prep_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """æµç¨‹é¢„å¤„ç†"""
        logger.info("ğŸš€ ========== å¼€å§‹ Web å•æ–‡ä»¶åˆ†ææµç¨‹ ==========")
        logger.info("ğŸ“‹ é˜¶æ®µ: æµç¨‹åˆå§‹åŒ– (WebAnalysisFlow.prep_async)")

        # éªŒè¯å¿…éœ€çš„è¾“å…¥å‚æ•°
        required_fields = ["task_id", "file_id", "vectorstore_index"]
        for field in required_fields:
            if field not in shared:
                logger.error(f"âŒ ç¼ºå°‘å¿…éœ€å‚æ•°: {field}")
                raise ValueError(f"Required field '{field}' is missing from shared data")

        task_id = shared.get("task_id")
        file_id = shared.get("file_id")
        vectorstore_index = shared.get("vectorstore_index")

        # éªŒè¯å‚æ•°ç±»å‹å’Œå€¼
        if not isinstance(task_id, int) or task_id <= 0:
            logger.error(f"âŒ ä»»åŠ¡IDæ— æ•ˆ: {task_id}")
            raise ValueError("Task ID must be a positive integer")

        if not isinstance(file_id, int) or file_id <= 0:
            logger.error(f"âŒ æ–‡ä»¶IDæ— æ•ˆ: {file_id}")
            raise ValueError("File ID must be a positive integer")

        if not vectorstore_index or not isinstance(vectorstore_index, str):
            logger.error(f"âŒ å‘é‡ç´¢å¼•æ— æ•ˆ: {vectorstore_index}")
            raise ValueError("Vectorstore index must be a valid string")

        logger.info(f"ğŸ¯ ä»»åŠ¡ID: {task_id}")
        logger.info(f"ğŸ“„ æ–‡ä»¶ID: {file_id}")
        logger.info(f"ğŸ“‚ å‘é‡ç´¢å¼•: {vectorstore_index}")

        # åˆå§‹åŒ–å…±äº«çŠ¶æ€
        shared.setdefault("status", "processing")
        shared["current_stage"] = "initialization"

        logger.info("âœ… æµç¨‹åˆå§‹åŒ–å®Œæˆ")
        return shared

    async def run_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•æ–‡ä»¶åˆ†ææµç¨‹"""
        try:
            # 1. é¢„å¤„ç†
            await self.prep_async(shared)

            # 2. è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = await self._get_file_info(shared["file_id"], shared["task_id"])
            if not file_info:
                shared["status"] = "failed"
                shared["error"] = "æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯"
                return shared

            # 3. æ‰§è¡Œåˆ†æ
            analysis_results = await self._analyze_file(file_info, shared["vectorstore_index"])

            # 4. ä¿å­˜åˆ†æç»“æœ
            await self._save_analysis_results(analysis_results, shared["task_id"])

            # 5. è®¾ç½®å®ŒæˆçŠ¶æ€å’Œåˆ†æé¡¹æ•°é‡
            detailed_analysis = analysis_results.get("detailed_analysis", [])
            analysis_items_count = 1 + len(detailed_analysis)  # 1ä¸ªå…¨å±€åˆ†æ + è¯¦ç»†åˆ†æé¡¹
            shared["status"] = "completed"
            shared["analysis_items_count"] = analysis_items_count

            # 6. åå¤„ç†
            await self.post_async(shared, {}, "")

            return shared

        except Exception as e:
            logger.error(f"âŒ å•æ–‡ä»¶åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            shared["status"] = "failed"
            shared["error"] = str(e)
            return shared

    async def post_async(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: str) -> Dict[str, Any]:
        """æµç¨‹åå¤„ç†"""
        logger.info("ğŸ“‹ é˜¶æ®µ: æµç¨‹åå¤„ç† (WebAnalysisFlow.post_async)")

        # é¿å…æœªä½¿ç”¨å‚æ•°è­¦å‘Š
        _ = prep_res, exec_res

        if shared.get("status") == "completed":
            logger.info(f"âœ… å•æ–‡ä»¶åˆ†ææµç¨‹å®Œæˆ")
        else:
            logger.error("âŒ å•æ–‡ä»¶åˆ†ææµç¨‹å¤±è´¥")

        logger.info("ğŸ ========== Web å•æ–‡ä»¶åˆ†ææµç¨‹ç»“æŸ ==========")
        return shared

    async def _get_file_info(self, file_id: int, task_id: int) -> Dict[str, Any]:
        """é€šè¿‡ GET /api/repository/file-analysis/{file_id} æ¥å£è·å–æ–‡ä»¶ä¿¡æ¯"""
        import aiohttp

        try:
            url = f"{self.api_base_url}/api/repository/file-analysis/{file_id}"
            params = {"task_id": task_id}

            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data.get("status") == "success":
                            file_analysis = data.get("file_analysis", {})
                            logger.info(f"âœ… æˆåŠŸè·å–æ–‡ä»¶ä¿¡æ¯: {file_analysis.get('file_path', 'unknown')}")
                            return file_analysis

                    logger.error(f"âŒ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: HTTP {response.status}")
                    return {}

        except Exception as e:
            logger.error(f"âŒ è·å–æ–‡ä»¶ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {}

    async def _analyze_file(self, file_info: Dict[str, Any], vectorstore_index: str) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶å†…å®¹ï¼Œç”Ÿæˆå…¨å±€åˆ†æå’Œç±»/å‡½æ•°åˆ†æ"""
        try:
            file_path = file_info.get("file_path", "")
            code_content = file_info.get("code_content", "")
            language = file_info.get("language", "")

            if not code_content:
                logger.warning(f"æ–‡ä»¶ {file_path} æ²¡æœ‰ä»£ç å†…å®¹ï¼Œåˆ›å»ºé»˜è®¤åˆ†æé¡¹")
                # ä¸ºç©ºæ–‡ä»¶åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†æé¡¹
                file_name = file_path.split("/")[-1] if "/" in file_path else file_path
                return {
                    "file_path": file_path,
                    "global_analysis": {
                        "title": f"{file_name} - ç©ºæ–‡ä»¶",
                        "description": "è¿™æ˜¯ä¸€ä¸ªç©ºæ–‡ä»¶ï¼Œæ²¡æœ‰ä»£ç å†…å®¹ã€‚é€šå¸¸ç”¨äºæ ‡è®° Python åŒ…ç›®å½•æˆ–ç±»å‹æç¤ºã€‚",
                        "target_type": "file",
                        "target_name": file_name,
                        "language": language or "unknown",
                    },
                    "detailed_analysis": [],
                }

            logger.info(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {file_path}")

            # 1. è·å–RAGä¸Šä¸‹æ–‡
            context = await self._get_rag_context(file_path, code_content, language, vectorstore_index)

            # 2. è¿›è¡Œå…¨å±€åˆ†æ
            global_analysis = await self._perform_global_analysis(file_path, code_content, language, context)

            # 3. è¿›è¡Œè¯¦ç»†åˆ†æï¼ˆç±»å’Œå‡½æ•°ï¼‰
            detailed_analysis = await self._perform_detailed_analysis(file_path, code_content, language, context)

            logger.info(f"âœ… å®Œæˆæ–‡ä»¶åˆ†æ: {file_path}")
            logger.info(f"   - å…¨å±€åˆ†æ: {global_analysis.get('title', 'N/A')}")
            logger.info(f"   - è¯¦ç»†åˆ†æé¡¹: {len(detailed_analysis)}")

            return {
                "file_path": file_path,
                "file_id": file_info.get("id"),  # æ·»åŠ  file_id
                "language": language,
                "code_content": code_content,  # æ·»åŠ  code_content
                "global_analysis": global_analysis,
                "detailed_analysis": detailed_analysis,
            }

        except Exception as e:
            logger.error(f"âŒ åˆ†ææ–‡ä»¶å¤±è´¥: {str(e)}")
            return {
                "file_path": file_info.get("file_path", ""),
                "global_analysis": {},
                "detailed_analysis": [],
                "error": str(e),
            }

    async def _get_rag_context(self, file_path: str, content: str, language: str, vectorstore_index: str) -> str:
        """è·å–RAGä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå‚è€ƒ CodeParsingBatchNode çš„å®ç°"""
        try:
            from ..utils.rag_api_client import RAGAPIClient
            from ..utils.config import get_config

            # é¿å…æœªä½¿ç”¨å‚æ•°è­¦å‘Š
            _ = content

            config = get_config()
            rag_client = RAGAPIClient(config.rag_base_url)

            # æ„å»ºå¤šä¸ªæŸ¥è¯¢ç­–ç•¥ï¼Œå‚è€ƒ CodeParsingBatchNode
            search_queries = []
            search_targets = []

            # 1. å¯¹æ–‡ä»¶æœ¬èº«è¿›è¡Œæ£€ç´¢
            search_queries.append(f"{file_path} {language} æ–‡ä»¶")
            search_targets.append(f"æ–‡ä»¶-{file_path}")

            # 2. å¯¹è¯­è¨€ç‰¹å®šçš„æ£€ç´¢
            search_queries.append(f"{language} ä»£ç åˆ†æ")
            search_targets.append(f"è¯­è¨€-{language}")

            logger.info(f"ğŸ” å¼€å§‹ä¸º {file_path} æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå…± {len(search_queries)} ä¸ªæŸ¥è¯¢")

            # 3. æ‰§è¡Œæ£€ç´¢ï¼Œæ”¶é›†æ‰€æœ‰ç»“æœ
            all_results = []
            for i, (query, target) in enumerate(zip(search_queries, search_targets), 1):
                try:
                    logger.info(f"   [{i}/{len(search_queries)}] æ£€ç´¢ {target}: {query}")
                    # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å search_knowledge
                    results = rag_client.search_knowledge(query=query, index_name=vectorstore_index, top_k=5)

                    found_count = 0
                    for result in results:
                        doc = result.get("document", {})
                        title = doc.get("title", "")
                        content_snippet = doc.get("content", "")

                        if title and content_snippet:
                            all_results.append(
                                {
                                    "title": title,
                                    "content": content_snippet,
                                    "file_path": doc.get("file_path", ""),
                                    "category": doc.get("category", ""),
                                    "language": doc.get("language", ""),
                                    "query": query,
                                    "search_target": target,
                                }
                            )
                            found_count += 1

                    logger.info(f"       æ‰¾åˆ° {found_count} ä¸ªç›¸å…³ç»“æœ")

                except Exception as e:
                    logger.warning(f"   [{i}/{len(search_queries)}] æ£€ç´¢å¤±è´¥ {target}: {str(e)}")
                    continue

            # 4. ç»„åˆæ£€ç´¢ç»“æœ
            if all_results:
                context_parts = ["=== RAG æ£€ç´¢ä¸Šä¸‹æ–‡ ==="]

                # æŒ‰æ£€ç´¢ç›®æ ‡åˆ†ç»„
                target_groups = {}
                for result in all_results[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªç»“æœ
                    target = result.get("search_target", "æœªçŸ¥ç›®æ ‡")
                    if target not in target_groups:
                        target_groups[target] = []
                    target_groups[target].append(result)

                # æŒ‰ç›®æ ‡åˆ†ç»„æ˜¾ç¤ºç»“æœ
                for target, results in target_groups.items():
                    context_parts.append(f"\n--- æ£€ç´¢ç›®æ ‡: {target} ---")
                    for i, result in enumerate(results[:3], 1):  # æ¯ä¸ªç›®æ ‡æœ€å¤š3ä¸ªç»“æœ
                        title = result.get("title", "Unknown")
                        content_snippet = result.get("content", "")
                        file_info = result.get("file_path", "")
                        category = result.get("category", "")

                        # æˆªå–åˆé€‚é•¿åº¦
                        snippet = content_snippet[:300] + "..." if len(content_snippet) > 300 else content_snippet

                        context_parts.append(f"  {i}. {title} ({category})")
                        if file_info:
                            context_parts.append(f"     æ–‡ä»¶: {file_info}")
                        context_parts.append(f"     {snippet}\n")

                context = "\n".join(context_parts)
                logger.info(
                    f"âœ… ä¸º {file_path} æ£€ç´¢åˆ° {len(all_results)} ä¸ªç›¸å…³ä¸Šä¸‹æ–‡ï¼Œåˆ†å¸ƒåœ¨ {len(target_groups)} ä¸ªæ£€ç´¢ç›®æ ‡ä¸­"
                )
                return context
            else:
                logger.info(f"âš ï¸ æœªæ‰¾åˆ° {file_path} çš„ç›¸å…³ä¸Šä¸‹æ–‡")
                return ""

        except Exception as e:
            logger.warning(f"âš ï¸ è·å–RAGä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            return ""

    async def _perform_global_analysis(
        self, file_path: str, code_content: str, language: str, context: str
    ) -> Dict[str, Any]:
        """å¯¹å®Œæ•´ä»£ç å†…å®¹è¿›è¡Œå…¨å±€åˆ†æ"""
        try:
            # æ„å»ºå…¨å±€åˆ†æçš„prompt
            prompt = f"""
è¯·å¯¹ä»¥ä¸‹{language}ä»£ç æ–‡ä»¶è¿›è¡Œå…¨å±€åˆ†æï¼Œç”Ÿæˆæ–‡ä»¶çº§åˆ«çš„æ ‡é¢˜å’Œæè¿°ã€‚

æ–‡ä»¶è·¯å¾„: {file_path}
ç¼–ç¨‹è¯­è¨€: {language}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

ä»£ç å†…å®¹:
```{language}
{code_content}
```

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›å…¨å±€åˆ†æç»“æœï¼š

{{
    "title": "æ–‡ä»¶çš„ç®€æ´æ ‡é¢˜ï¼ˆå¦‚ï¼šç”¨æˆ·ç®¡ç†æ¨¡å— æˆ– æ•°æ®åº“è¿æ¥å·¥å…·ï¼‰",
    "description": "è¯¦ç»†çš„æ–‡ä»¶åŠŸèƒ½æè¿°ï¼ŒåŒ…æ‹¬ä¸»è¦ç”¨é€”ã€æ ¸å¿ƒåŠŸèƒ½ã€è®¾è®¡ç›®æ ‡ç­‰ï¼ˆ3-5å¥ä¸“ä¸šæè¿°ï¼‰"
}}

åˆ†æè¦æ±‚ï¼š
1. TITLE: ç®€æ´æ˜ç¡®ï¼Œä½“ç°æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½æˆ–ç”¨é€”
2. DESCRIPTION: è¯¦ç»†ä¸“ä¸šçš„æè¿°ï¼ŒåŒ…æ‹¬ï¼š
   - æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”
   - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘æˆ–æŠ€æœ¯å®ç°
   - åœ¨æ•´ä¸ªé¡¹ç›®ä¸­çš„ä½œç”¨
   - ä¸»è¦çš„ç±»å’Œå‡½æ•°æ¦‚è¿°

åªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

            # è°ƒç”¨LLM API
            response = await self.llm_parser._make_api_request(prompt)

            # è§£æå“åº”
            import json

            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()

            result = json.loads(clean_response)

            logger.info(f"âœ… å®Œæˆå…¨å±€åˆ†æ: {result.get('title', 'N/A')}")
            return result

        except Exception as e:
            logger.error(f"âŒ å…¨å±€åˆ†æå¤±è´¥: {str(e)}")
            return {"title": f"{file_path} æ–‡ä»¶", "description": "æ–‡ä»¶åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæè¿°"}

    async def _perform_detailed_analysis(
        self, file_path: str, code_content: str, language: str, context: str
    ) -> List[Dict[str, Any]]:
        """å¯¹ä»£ç ä¸­çš„å®Œæ•´ç±»å’Œç‹¬ç«‹å‡½æ•°è¿›è¡Œè¯¦ç»†åˆ†æ"""
        try:
            # è§£æä»£ç ç»“æ„ï¼Œæå–ç±»å’Œç‹¬ç«‹å‡½æ•°
            code_elements = self._parse_code_structure(code_content, language)

            analysis_items = []

            # åˆ†ææ¯ä¸ªä»£ç å…ƒç´ 
            for element in code_elements:
                if element["type"] == "class":
                    # åˆ†æå®Œæ•´çš„ç±»ï¼ˆåŒ…å«æ‰€æœ‰æ–¹æ³•ï¼‰
                    class_analysis = await self._analyze_complete_class(element, file_path, language, context)
                    if class_analysis:
                        analysis_items.append(class_analysis)

                elif element["type"] == "function":
                    # åˆ†æç‹¬ç«‹å‡½æ•°
                    function_analysis = await self._analyze_independent_function(element, file_path, language, context)
                    if function_analysis:
                        analysis_items.append(function_analysis)

            logger.info(f"âœ… å®Œæˆè¯¦ç»†åˆ†æï¼Œç”Ÿæˆ {len(analysis_items)} ä¸ªåˆ†æé¡¹")
            logger.info(
                f"   - ç±»: {len([item for item in analysis_items if 'class' in item.get('title', '').lower()])} ä¸ª"
            )
            logger.info(
                f"   - ç‹¬ç«‹å‡½æ•°: {len([item for item in analysis_items if 'function' in item.get('title', '').lower()])} ä¸ª"
            )

            return analysis_items

        except Exception as e:
            logger.error(f"âŒ è¯¦ç»†åˆ†æå¤±è´¥: {str(e)}")
            return []

    def _parse_code_structure(self, code_content: str, language: str) -> List[Dict[str, Any]]:
        """è§£æä»£ç ç»“æ„ï¼Œæå–ç±»å’Œç‹¬ç«‹å‡½æ•°"""
        import ast
        import re

        elements = []

        if language.lower() == "python":
            try:
                tree = ast.parse(code_content)
                lines = code_content.split("\n")

                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        # æå–å®Œæ•´çš„ç±»ï¼ˆåŒ…å«æ‰€æœ‰æ–¹æ³•ï¼‰
                        start_line = node.lineno
                        end_line = self._find_class_end_line(node, lines)
                        class_code = "\n".join(lines[start_line - 1 : end_line])

                        elements.append(
                            {
                                "type": "class",
                                "name": node.name,
                                "start_line": start_line,
                                "end_line": end_line,
                                "code": class_code,
                                "methods": [method.name for method in node.body if isinstance(method, ast.FunctionDef)],
                            }
                        )

                    elif isinstance(node, ast.FunctionDef):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‹¬ç«‹å‡½æ•°ï¼ˆä¸åœ¨ç±»ä¸­ï¼‰
                        if self._is_independent_function(node, tree):
                            # æ’é™¤ç¨‹åºå…¥å£
                            if node.name != "__main__" and not self._is_main_guard_function(node, lines):
                                start_line = node.lineno
                                end_line = self._find_function_end_line(node, lines)
                                function_code = "\n".join(lines[start_line - 1 : end_line])

                                elements.append(
                                    {
                                        "type": "function",
                                        "name": node.name,
                                        "start_line": start_line,
                                        "end_line": end_line,
                                        "code": function_code,
                                    }
                                )

            except SyntaxError as e:
                logger.warning(f"âš ï¸ Python ä»£ç è§£æå¤±è´¥: {str(e)}")
                # å¦‚æœASTè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                elements = self._parse_with_regex(code_content, language)
        else:
            # å¯¹äºå…¶ä»–è¯­è¨€ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ
            elements = self._parse_with_regex(code_content, language)

        logger.info(
            f"ğŸ” è§£æä»£ç ç»“æ„å®Œæˆ: æ‰¾åˆ° {len([e for e in elements if e['type'] == 'class'])} ä¸ªç±», {len([e for e in elements if e['type'] == 'function'])} ä¸ªç‹¬ç«‹å‡½æ•°"
        )
        return elements

    def _find_class_end_line(self, class_node, lines: List[str]) -> int:
        """æ‰¾åˆ°ç±»çš„ç»“æŸè¡Œï¼ˆåŒ…æ‹¬æ‰€æœ‰æ–¹æ³•ï¼‰"""
        import ast

        # ä½¿ç”¨ASTèŠ‚ç‚¹ä¿¡æ¯æ‰¾åˆ°ç±»çš„çœŸæ­£ç»“æŸä½ç½®
        if hasattr(class_node, "end_lineno") and class_node.end_lineno:
            return class_node.end_lineno

        # å¦‚æœæ²¡æœ‰end_linenoï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•
        start_line = class_node.lineno
        class_indent = self._get_line_indent(lines[start_line - 1])

        # ä»ç±»å®šä¹‰çš„ä¸‹ä¸€è¡Œå¼€å§‹æŸ¥æ‰¾
        for i in range(start_line, len(lines)):
            line = lines[i]
            if line.strip():  # éç©ºè¡Œ
                current_indent = self._get_line_indent(line)
                # å¦‚æœé‡åˆ°åŒçº§åˆ«æˆ–æ›´ä½çº§åˆ«çš„ä»£ç ï¼ˆä¸æ˜¯ç±»çš„å†…å®¹ï¼‰ï¼Œåˆ™ç±»ç»“æŸ
                if current_indent <= class_indent:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„ç±»ã€å‡½æ•°æˆ–å…¶ä»–é¡¶çº§å®šä¹‰
                    stripped = line.strip()
                    if (
                        stripped.startswith("class ")
                        or stripped.startswith("def ")
                        or stripped.startswith("if __name__")
                        or stripped.startswith("import ")
                        or stripped.startswith("from ")
                    ):
                        return i

        return len(lines)

    def _get_line_indent(self, line: str) -> int:
        """è·å–è¡Œçš„ç¼©è¿›çº§åˆ«"""
        indent = 0
        for char in line:
            if char == " ":
                indent += 1
            elif char == "\t":
                indent += 4  # å‡è®¾tabç­‰äº4ä¸ªç©ºæ ¼
            else:
                break
        return indent

    def _find_function_end_line(self, func_node, lines: List[str]) -> int:
        """æ‰¾åˆ°å‡½æ•°çš„ç»“æŸè¡Œ"""
        # ç®€å•å®ç°ï¼šæ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒçº§åˆ«çš„å®šä¹‰æˆ–æ–‡ä»¶ç»“æŸ
        start_line = func_node.lineno
        for i in range(start_line, len(lines)):
            line = lines[i].strip()
            if line and not line.startswith(" ") and not line.startswith("\t") and not line.startswith("#"):
                if line.startswith("class ") or line.startswith("def ") or line.startswith("if __name__"):
                    return i
        return len(lines)

    def _is_independent_function(self, func_node, tree) -> bool:
        """æ£€æŸ¥å‡½æ•°æ˜¯å¦æ˜¯ç‹¬ç«‹å‡½æ•°ï¼ˆä¸åœ¨ç±»ä¸­ï¼‰"""
        import ast

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                for item in node.body:
                    if item is func_node:
                        return False
        return True

    def _is_main_guard_function(self, func_node, lines: List[str]) -> bool:
        """æ£€æŸ¥å‡½æ•°æ˜¯å¦åœ¨ if __name__ == "__main__": å—ä¸­"""
        func_line = func_node.lineno - 1
        # å‘ä¸ŠæŸ¥æ‰¾æ˜¯å¦æœ‰ if __name__ == "__main__":
        for i in range(func_line - 1, -1, -1):
            line = lines[i].strip()
            if "if __name__" in line and "__main__" in line:
                return True
            elif line and not line.startswith(" ") and not line.startswith("\t") and not line.startswith("#"):
                break
        return False

    def _parse_with_regex(self, code_content: str, language: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æä»£ç ç»“æ„ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        import re

        elements = []
        lines = code_content.split("\n")

        if language.lower() == "python":
            # åŒ¹é…ç±»å®šä¹‰
            class_pattern = r"^class\s+(\w+).*?:"
            for i, line in enumerate(lines):
                if re.match(class_pattern, line.strip()):
                    class_name = re.match(class_pattern, line.strip()).group(1)
                    start_line = i + 1
                    end_line = self._find_block_end_regex(lines, i)
                    class_code = "\n".join(lines[i:end_line])

                    elements.append(
                        {
                            "type": "class",
                            "name": class_name,
                            "start_line": start_line,
                            "end_line": end_line,
                            "code": class_code,
                            "methods": [],
                        }
                    )

            # åŒ¹é…ç‹¬ç«‹å‡½æ•°å®šä¹‰
            func_pattern = r"^def\s+(\w+).*?:"
            for i, line in enumerate(lines):
                if re.match(func_pattern, line.strip()) and not self._is_in_class_regex(lines, i):
                    func_name = re.match(func_pattern, line.strip()).group(1)
                    if not self._is_main_guard_regex(lines, i):
                        start_line = i + 1
                        end_line = self._find_block_end_regex(lines, i)
                        func_code = "\n".join(lines[i:end_line])

                        elements.append(
                            {
                                "type": "function",
                                "name": func_name,
                                "start_line": start_line,
                                "end_line": end_line,
                                "code": func_code,
                            }
                        )

        return elements

    def _find_block_end_regex(self, lines: List[str], start_index: int) -> int:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°ä»£ç å—çš„ç»“æŸä½ç½®ï¼ˆæ”¹è¿›ç‰ˆï¼Œæ”¯æŒç±»çš„å®Œæ•´ç»“æ„ï¼‰"""
        if start_index >= len(lines):
            return len(lines)

        # è·å–èµ·å§‹è¡Œçš„ç¼©è¿›çº§åˆ«
        start_line = lines[start_index]
        start_indent = self._get_line_indent(start_line)

        # ä»ä¸‹ä¸€è¡Œå¼€å§‹æŸ¥æ‰¾
        for i in range(start_index + 1, len(lines)):
            line = lines[i]
            if line.strip():  # éç©ºè¡Œ
                current_indent = self._get_line_indent(line)
                # å¦‚æœé‡åˆ°åŒçº§åˆ«æˆ–æ›´ä½çº§åˆ«çš„ä»£ç ï¼Œåˆ™å—ç»“æŸ
                if current_indent <= start_indent:
                    stripped = line.strip()
                    if (
                        stripped.startswith("class ")
                        or stripped.startswith("def ")
                        or stripped.startswith("if __name__")
                        or stripped.startswith("import ")
                        or stripped.startswith("from ")
                    ):
                        return i

        return len(lines)

    def _is_in_class_regex(self, lines: List[str], func_index: int) -> bool:
        """æ£€æŸ¥å‡½æ•°æ˜¯å¦åœ¨ç±»ä¸­"""
        # å‘ä¸ŠæŸ¥æ‰¾æœ€è¿‘çš„ç±»å®šä¹‰
        for i in range(func_index - 1, -1, -1):
            line = lines[i].strip()
            if line.startswith("class "):
                return True
            elif line and not line.startswith(" ") and not line.startswith("\t") and not line.startswith("#"):
                if line.startswith("def ") or line.startswith("if __name__"):
                    break
        return False

    def _is_main_guard_regex(self, lines: List[str], func_index: int) -> bool:
        """æ£€æŸ¥å‡½æ•°æ˜¯å¦åœ¨ if __name__ == "__main__": å—ä¸­"""
        for i in range(func_index - 1, -1, -1):
            line = lines[i].strip()
            if "if __name__" in line and "__main__" in line:
                return True
            elif line and not line.startswith(" ") and not line.startswith("\t") and not line.startswith("#"):
                break
        return False

    async def _analyze_complete_class(
        self, class_element: Dict[str, Any], file_path: str, language: str, context: str
    ) -> Dict[str, Any]:
        """åˆ†æå®Œæ•´çš„ç±»ï¼ˆåŒ…å«æ‰€æœ‰æ–¹æ³•ï¼‰"""
        try:
            class_name = class_element["name"]
            class_code = class_element["code"]
            methods = class_element.get("methods", [])

            # æ„å»ºåˆ†ææç¤º
            prompt = f"""
è¯·å¯¹ä»¥ä¸‹{language}ç±»è¿›è¡Œå®Œæ•´åˆ†æï¼Œç”Ÿæˆç±»çº§åˆ«çš„æ ‡é¢˜å’Œæè¿°ã€‚

ç±»å: {class_name}
æ–‡ä»¶è·¯å¾„: {file_path}
åŒ…å«æ–¹æ³•: {', '.join(methods) if methods else 'æ— '}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

ç±»ä»£ç :
```{language}
{class_code}
```

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
    "title": "ç±»çš„ç®€æ´æ ‡é¢˜ï¼ˆå¦‚ï¼šUserServiceç±» æˆ– æ•°æ®å¤„ç†å™¨ç±»ï¼‰",
    "description": "è¯¦ç»†çš„ç±»åŠŸèƒ½æè¿°ï¼ŒåŒ…æ‹¬ç±»çš„ç”¨é€”ã€ä¸»è¦æ–¹æ³•ã€è®¾è®¡æ¨¡å¼ç­‰ï¼ˆ3-5å¥ä¸“ä¸šæè¿°ï¼‰"
}}

åˆ†æè¦æ±‚ï¼š
1. TITLE: ç®€æ´æ˜ç¡®ï¼Œä½“ç°ç±»çš„ä¸»è¦åŠŸèƒ½
2. DESCRIPTION: è¯¦ç»†æè¿°ç±»çš„åŠŸèƒ½ã€åŒ…å«çš„ä¸»è¦æ–¹æ³•ã€åœ¨ç³»ç»Ÿä¸­çš„ä½œç”¨

åªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

            # è°ƒç”¨LLM API
            response = await self.llm_parser._make_api_request(prompt)

            # è§£æå“åº”
            import json

            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()

            result = json.loads(clean_response)

            # æ·»åŠ é¢å¤–ä¿¡æ¯
            result.update(
                {
                    "source": f"{file_path}:{class_element['start_line']}-{class_element['end_line']}",
                    "code": class_code,
                    "start_line": class_element["start_line"],
                    "end_line": class_element["end_line"],
                    "original_name": class_element["name"],  # æ·»åŠ åŸå§‹ç±»å
                    "element_type": "class",  # æ·»åŠ å…ƒç´ ç±»å‹
                }
            )

            logger.info(f"âœ… å®Œæˆç±»åˆ†æ: {result.get('title', 'N/A')}")
            return result

        except Exception as e:
            logger.error(f"âŒ ç±»åˆ†æå¤±è´¥: {str(e)}")
            return {
                "title": f"{class_element['name']}ç±»",
                "description": "ç±»åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæè¿°",
                "source": f"{file_path}:{class_element['start_line']}-{class_element['end_line']}",
                "code": class_element["code"],
                "start_line": class_element["start_line"],
                "end_line": class_element["end_line"],
            }

    async def _analyze_independent_function(
        self, func_element: Dict[str, Any], file_path: str, language: str, context: str
    ) -> Dict[str, Any]:
        """åˆ†æç‹¬ç«‹å‡½æ•°"""
        try:
            func_name = func_element["name"]
            func_code = func_element["code"]

            # æ„å»ºåˆ†ææç¤º
            prompt = f"""
è¯·å¯¹ä»¥ä¸‹{language}ç‹¬ç«‹å‡½æ•°è¿›è¡Œåˆ†æï¼Œç”Ÿæˆå‡½æ•°çº§åˆ«çš„æ ‡é¢˜å’Œæè¿°ã€‚

å‡½æ•°å: {func_name}
æ–‡ä»¶è·¯å¾„: {file_path}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

å‡½æ•°ä»£ç :
```{language}
{func_code}
```

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
    "title": "å‡½æ•°çš„ç®€æ´æ ‡é¢˜ï¼ˆå¦‚ï¼šæ•°æ®é¢„å¤„ç†å‡½æ•° æˆ– ç”¨æˆ·éªŒè¯æ–¹æ³•ï¼‰",
    "description": "è¯¦ç»†çš„å‡½æ•°åŠŸèƒ½æè¿°ï¼ŒåŒ…æ‹¬å‚æ•°ã€è¿”å›å€¼ã€ä¸»è¦é€»è¾‘ç­‰ï¼ˆ2-4å¥ä¸“ä¸šæè¿°ï¼‰"
}}

åˆ†æè¦æ±‚ï¼š
1. TITLE: ç®€æ´æ˜ç¡®ï¼Œä½“ç°å‡½æ•°çš„ä¸»è¦åŠŸèƒ½
2. DESCRIPTION: è¯¦ç»†æè¿°å‡½æ•°çš„åŠŸèƒ½ã€å‚æ•°ã€è¿”å›å€¼ã€ä¸»è¦é€»è¾‘

åªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

            # è°ƒç”¨LLM API
            response = await self.llm_parser._make_api_request(prompt)

            # è§£æå“åº”
            import json

            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()

            result = json.loads(clean_response)

            # æ·»åŠ é¢å¤–ä¿¡æ¯
            result.update(
                {
                    "source": f"{file_path}:{func_element['start_line']}-{func_element['end_line']}",
                    "code": func_code,
                    "start_line": func_element["start_line"],
                    "end_line": func_element["end_line"],
                    "original_name": func_element["name"],  # æ·»åŠ åŸå§‹å‡½æ•°å
                    "element_type": "function",  # æ·»åŠ å…ƒç´ ç±»å‹
                }
            )

            logger.info(f"âœ… å®Œæˆå‡½æ•°åˆ†æ: {result.get('title', 'N/A')}")
            return result

        except Exception as e:
            logger.error(f"âŒ å‡½æ•°åˆ†æå¤±è´¥: {str(e)}")
            return {
                "title": f"{func_element['name']}å‡½æ•°",
                "description": "å‡½æ•°åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæè¿°",
                "source": f"{file_path}:{func_element['start_line']}-{func_element['end_line']}",
                "code": func_element["code"],
                "start_line": func_element["start_line"],
                "end_line": func_element["end_line"],
            }

    async def _save_analysis_results(self, analysis_results: Dict[str, Any], task_id: int):
        """é€šè¿‡ POST /api/repository/analysis-items æ¥å£ä¿å­˜åˆ†æç»“æœï¼Œå¹¶æ›´æ–° file_analyses è¡¨"""
        import aiohttp

        try:
            file_path = analysis_results.get("file_path", "")
            language = analysis_results.get("language", "")
            global_analysis = analysis_results.get("global_analysis", {})
            detailed_analysis = analysis_results.get("detailed_analysis", [])

            # é¦–å…ˆéœ€è¦è·å– file_analysis_idï¼ˆä» _get_file_info è·å–çš„ file_idï¼‰
            file_analysis_id = analysis_results.get("file_id")
            if not file_analysis_id:
                logger.error(f"âŒ æ— æ³•è·å–æ–‡ä»¶åˆ†æID: {file_path}")
                return

            # 1. ä¿å­˜å…¨å±€åˆ†æç»“æœï¼ˆæ–‡ä»¶çº§åˆ«ï¼‰
            if global_analysis:
                # è®¡ç®—æ–‡ä»¶çš„æ€»è¡Œæ•°
                code_content = analysis_results.get("code_content", "")
                total_lines = len(code_content.split("\n")) if code_content else 0

                # æå–æ–‡ä»¶åä½œä¸º target_name
                import os

                file_name = os.path.basename(file_path)

                global_item_data = {
                    "file_analysis_id": file_analysis_id,
                    "title": global_analysis.get("title", ""),
                    "description": global_analysis.get("description", ""),
                    "target_type": "file",
                    "target_name": file_name,  # æ–‡ä»¶å
                    "source": file_path,
                    "language": language,
                    "code": code_content,  # å®Œæ•´çš„æ–‡ä»¶ä»£ç 
                    "start_line": 1,  # æ–‡ä»¶ä»ç¬¬1è¡Œå¼€å§‹
                    "end_line": total_lines,  # æ–‡ä»¶çš„æœ€åä¸€è¡Œ
                }

                await self._post_analysis_item(global_item_data)
                logger.info(f"âœ… ä¿å­˜å…¨å±€åˆ†æ: {global_analysis.get('title', 'N/A')}")

            # 2. ä¿å­˜è¯¦ç»†åˆ†æç»“æœï¼ˆç±»å’Œå‡½æ•°ï¼‰
            for item in detailed_analysis:
                target_type = self._infer_target_type(item.get("title", ""))

                # ä»åˆ†æç»“æœä¸­ç›´æ¥è·å–ç›®æ ‡åç§°ï¼Œæˆ–è€…ä»ä»£ç ä¸­æå–
                target_name = self._get_target_name_from_analysis(item, target_type)

                # ç¡®ä¿ä»£ç ç‰‡æ®µå®Œæ•´
                code_snippet = item.get("code", "")
                if not code_snippet:
                    # å¦‚æœæ²¡æœ‰ä»£ç ç‰‡æ®µï¼Œå°è¯•ä»æ–‡ä»¶ä¸­æå–
                    code_snippet = self._extract_code_snippet(
                        analysis_results.get("code_content", ""), item.get("start_line"), item.get("end_line")
                    )

                detail_item_data = {
                    "file_analysis_id": file_analysis_id,
                    "title": item.get("title", ""),
                    "description": item.get("description", ""),
                    "target_type": target_type,
                    "target_name": target_name,  # ç¡®ä¿æœ‰ç›®æ ‡åç§°
                    "source": item.get("source", file_path),
                    "language": language,
                    "code": code_snippet,  # å®Œæ•´çš„ä»£ç ç‰‡æ®µ
                    "start_line": item.get("start_line"),  # å¦‚å®å¡«å†™èµ·å§‹è¡Œ
                    "end_line": item.get("end_line"),  # å¦‚å®å¡«å†™ç»“æŸè¡Œ
                }

                await self._post_analysis_item(detail_item_data)
                logger.info(f"âœ… ä¿å­˜è¯¦ç»†åˆ†æ: {item.get('title', 'N/A')}")

            # 3. æ›´æ–° file_analyses è¡¨çš„çŠ¶æ€å’Œåˆ†æç»“æœ
            await self._update_file_analysis_status(file_analysis_id, detailed_analysis)

            logger.info(f"âœ… å®Œæˆä¿å­˜åˆ†æç»“æœï¼Œå…± {1 + len(detailed_analysis)} é¡¹")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}")
            raise

    async def _post_analysis_item(self, data: Dict[str, Any]):
        """è°ƒç”¨ POST /api/repository/analysis-items æ¥å£"""
        import aiohttp

        try:
            url = f"{self.api_base_url}/api/repository/analysis-items"

            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=data) as response:
                    if response.status == 201:
                        result = await response.json()
                        if result.get("status") == "success":
                            return result

                    logger.error(f"âŒ ä¿å­˜åˆ†æé¡¹å¤±è´¥: HTTP {response.status}")
                    error_data = await response.json() if response.content_type == "application/json" else {}
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {error_data}")

        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨åˆ†æé¡¹æ¥å£å¤±è´¥: {str(e)}")
            raise

    async def _update_file_analysis_status(self, file_id: int, detailed_analysis: List[Dict[str, Any]]):
        """ä½¿ç”¨ PUT /api/repository/file-analysis/{file_id} æ¥å£æ›´æ–°æ–‡ä»¶åˆ†æçŠ¶æ€"""
        import aiohttp

        try:
            # ç»Ÿè®¡åˆ†æé¡¹ä¿¡æ¯
            classes = []
            functions = []

            for item in detailed_analysis:
                # ä¼˜å…ˆä½¿ç”¨åŸå§‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ ‡é¢˜ä¸­æå–
                original_name = item.get("original_name")
                element_type = item.get("element_type")

                if original_name and element_type:
                    # ä½¿ç”¨å‡†ç¡®çš„åŸå§‹åç§°å’Œç±»å‹
                    if element_type == "class":
                        classes.append(original_name)
                    elif element_type == "function":
                        functions.append(original_name)
                else:
                    # å›é€€åˆ°ä»æ ‡é¢˜ä¸­æå–ï¼ˆå…¼å®¹æ€§ï¼‰
                    target_type = self._infer_target_type(item.get("title", ""))
                    target_name = self._extract_target_name(item.get("title", ""), target_type)

                    if target_type == "class" and target_name:
                        classes.append(target_name)
                    elif target_type == "function" and target_name:
                        functions.append(target_name)

            # æ„å»º file_analysis å†…å®¹ï¼ˆä¸åŒ…å«æ–‡ä»¶çº§åˆ«çš„åˆ†æé¡¹ï¼‰
            total_items = len(detailed_analysis)  # åªç»Ÿè®¡ç±»å’Œå‡½æ•°çš„åˆ†æé¡¹

            if total_items == 0:
                analysis_summary = "æ–‡ä»¶åŒ…å« 0 ä¸ªåˆ†æé¡¹"
            else:
                # æ„å»ºå…·ä½“çš„ç±»åå’Œå‡½æ•°ååˆ—è¡¨
                item_names = []

                # æ·»åŠ ç±»å
                for class_name in classes:
                    item_names.append(f"{class_name}ç±»")

                # æ·»åŠ å‡½æ•°å
                for func_name in functions:
                    item_names.append(f"{func_name}å‡½æ•°")

                # æ„å»ºæ‘˜è¦
                if item_names:
                    analysis_summary = f"æ–‡ä»¶åŒ…å« {total_items} ä¸ªåˆ†æé¡¹ï¼š{', '.join(item_names)}"
                else:
                    analysis_summary = f"æ–‡ä»¶åŒ…å« {total_items} ä¸ªåˆ†æé¡¹"

            # å‡†å¤‡æ›´æ–°æ•°æ®
            update_data = {
                "status": "success",
                "file_analysis": analysis_summary,  # ç›´æ¥ä½¿ç”¨æ‘˜è¦æ–‡æœ¬
            }

            # è°ƒç”¨ PUT æ¥å£æ›´æ–°
            url = f"{self.api_base_url}/api/repository/file-analysis/{file_id}"

            async with aiohttp.ClientSession() as session:
                async with session.put(url, json=update_data) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get("status") == "success":
                            logger.info(f"âœ… æ›´æ–°æ–‡ä»¶åˆ†æçŠ¶æ€æˆåŠŸ: {analysis_summary}")
                            return result

                    logger.error(f"âŒ æ›´æ–°æ–‡ä»¶åˆ†æçŠ¶æ€å¤±è´¥: HTTP {response.status}")
                    error_data = await response.json() if response.content_type == "application/json" else {}
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {error_data}")

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ–‡ä»¶åˆ†æçŠ¶æ€å¤±è´¥: {str(e)}")
            raise

    def _infer_target_type(self, title: str) -> str:
        """ä»æ ‡é¢˜æ¨æ–­ç›®æ ‡ç±»å‹"""
        title_lower = title.lower()

        if "class" in title_lower or "ç±»" in title:
            return "class"
        elif any(keyword in title_lower for keyword in ["function", "method", "def", "å‡½æ•°", "æ–¹æ³•"]):
            return "function"
        else:
            return "file"

    def _extract_target_name(self, title: str, target_type: str) -> str:
        """ä»æ ‡é¢˜ä¸­æå–ç›®æ ‡åç§°ï¼ˆç±»åæˆ–å‡½æ•°åï¼‰"""
        import re

        if target_type == "class":
            # å°è¯•æå–ç±»å
            patterns = [
                r"class\s+([A-Za-z_][A-Za-z0-9_]*)",  # class ClassName
                r"([A-Za-z_][A-Za-z0-9_]*)\s*ç±»",  # ClassNameç±»
                r"ç±»\s*([A-Za-z_][A-Za-z0-9_]*)",  # ç±» ClassName
            ]
            for pattern in patterns:
                match = re.search(pattern, title, re.IGNORECASE)
                if match:
                    return match.group(1)

        elif target_type == "function":
            # å°è¯•æå–å‡½æ•°å
            patterns = [
                r"def\s+([A-Za-z_][A-Za-z0-9_]*)",  # def function_name
                r"function\s+([A-Za-z_][A-Za-z0-9_]*)",  # function function_name
                r"([A-Za-z_][A-Za-z0-9_]*)\s*å‡½æ•°",  # function_nameå‡½æ•°
                r"å‡½æ•°\s*([A-Za-z_][A-Za-z0-9_]*)",  # å‡½æ•° function_name
                r"æ–¹æ³•\s*([A-Za-z_][A-Za-z0-9_]*)",  # æ–¹æ³• method_name
                r"([A-Za-z_][A-Za-z0-9_]*)\s*æ–¹æ³•",  # method_nameæ–¹æ³•
            ]
            for pattern in patterns:
                match = re.search(pattern, title, re.IGNORECASE)
                if match:
                    return match.group(1)

        # å¦‚æœæ— æ³•æå–ï¼Œè¿”å› None
        return None

    def _get_target_name_from_analysis(self, item: Dict[str, Any], target_type: str) -> str:
        """ä»åˆ†æç»“æœä¸­è·å–ç›®æ ‡åç§°"""
        # 1. é¦–å…ˆå°è¯•ä»åˆ†æç»“æœä¸­ç›´æ¥è·å–
        if "target_name" in item and item["target_name"]:
            return item["target_name"]

        # 2. ä»æ ‡é¢˜ä¸­æå–
        title = item.get("title", "")
        extracted_name = self._extract_target_name(title, target_type)
        if extracted_name:
            return extracted_name

        # 3. ä»ä»£ç ä¸­æå–ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        code = item.get("code", "")
        if code:
            if target_type == "class":
                # ä»ä»£ç ä¸­æå–ç±»å
                import re

                class_match = re.search(r"class\s+([A-Za-z_][A-Za-z0-9_]*)", code)
                if class_match:
                    return class_match.group(1)
            elif target_type == "function":
                # ä»ä»£ç ä¸­æå–å‡½æ•°å
                import re

                func_match = re.search(r"def\s+([A-Za-z_][A-Za-z0-9_]*)", code)
                if func_match:
                    return func_match.group(1)

        # 4. å¦‚æœéƒ½æ— æ³•æå–ï¼Œè¿”å›é»˜è®¤å€¼
        if target_type == "class":
            return "UnknownClass"
        elif target_type == "function":
            return "UnknownFunction"
        else:
            return "Unknown"

    def _extract_code_snippet(self, full_code: str, start_line: int, end_line: int) -> str:
        """ä»å®Œæ•´ä»£ç ä¸­æå–æŒ‡å®šè¡ŒèŒƒå›´çš„ä»£ç ç‰‡æ®µ"""
        if not full_code or not start_line or not end_line:
            return ""

        try:
            lines = full_code.split("\n")
            # è½¬æ¢ä¸º0åŸºç´¢å¼•
            start_idx = max(0, start_line - 1)
            end_idx = min(len(lines), end_line)

            # æå–ä»£ç ç‰‡æ®µ
            code_snippet = "\n".join(lines[start_idx:end_idx])
            return code_snippet

        except Exception as e:
            logger.warning(f"âš ï¸ æå–ä»£ç ç‰‡æ®µå¤±è´¥: {str(e)}")
            return ""


async def analyze_data_model(
    task_id: int, vectorstore_index: str, batch_size: int = None, progress_callback=None
) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®æ¨¡å‹çš„ä¾¿æ·å‡½æ•° - å¼‚æ­¥ç‰ˆæœ¬ï¼šåªæäº¤ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ

    Args:
        task_id: ä»»åŠ¡ID
        vectorstore_index: å‘é‡ç´¢å¼•åç§°
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼Œä½†åœ¨æ–°æµç¨‹ä¸­ä¸ä½¿ç”¨ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    import aiohttp
    from ..utils.config import get_config

    # é¿å…æœªä½¿ç”¨å‚æ•°è­¦å‘Š
    _ = batch_size

    config = get_config()
    api_base_url = config.api_base_url

    logger.info("ğŸ ========== å¼€å§‹æäº¤æ–‡ä»¶åˆ†æä»»åŠ¡ï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰==========")

    try:
        # 1. å…ˆè·å–ä»»åŠ¡ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        logger.info(f"ğŸ“‹ æ­¥éª¤ 1: è·å–ä»»åŠ¡ {task_id} ä¸‹çš„æ‰€æœ‰æ–‡ä»¶")

        async with aiohttp.ClientSession() as session:
            url = f"{api_base_url}/api/repository/files/{task_id}"
            params = {"include_code_content": "false"}  # åªè·å–æ–‡ä»¶åˆ—è¡¨ï¼Œä¸éœ€è¦å†…å®¹

            async with session.get(url, params=params) as response:
                if response.status != 200:
                    error_msg = f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: HTTP {response.status}"
                    logger.error(error_msg)
                    return {"status": "failed", "error": error_msg, "task_id": task_id}

                data = await response.json()
                if data.get("status") != "success":
                    error_msg = f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"
                    logger.error(error_msg)
                    return {"status": "failed", "error": error_msg, "task_id": task_id}

                files = data.get("files", [])

        if not files:
            logger.warning(f"ä»»åŠ¡ {task_id} ä¸‹æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            return {
                "status": "analysis_completed",
                "task_id": task_id,
                "analysis_items_count": 0,
                "message": "æ²¡æœ‰æ–‡ä»¶éœ€è¦åˆ†æ",
            }

        logger.info(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")

        # 2. è¿‡æ»¤å‡º pending çŠ¶æ€çš„æ–‡ä»¶ï¼ˆè·³è¿‡å·²å®Œæˆçš„æ–‡ä»¶ï¼Œé¿å…é‡å¤åˆ†æï¼‰
        pending_files = [f for f in files if f.get("status") == "pending"]
        skipped_files = len(files) - len(pending_files)

        if skipped_files > 0:
            logger.info(f"â­ï¸  è·³è¿‡ {skipped_files} ä¸ªå·²å®Œæˆçš„æ–‡ä»¶")

        if not pending_files:
            logger.info(f"âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²åˆ†æå®Œæˆï¼Œæ— éœ€é‡å¤æäº¤")
            return {
                "status": "analysis_completed",
                "task_id": task_id,
                "analysis_items_count": 0,
                "message": "æ‰€æœ‰æ–‡ä»¶éƒ½å·²åˆ†æå®Œæˆ",
            }

        logger.info(f"ğŸ“‹ éœ€è¦åˆ†æçš„æ–‡ä»¶æ•°: {len(pending_files)}")

        # 3. æ‰¹é‡æäº¤ pending çŠ¶æ€çš„æ–‡ä»¶åˆ†æä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼Œä¸ç­‰å¾…å®Œæˆï¼‰
        total_files = len(pending_files)
        submitted_files = 0
        failed_submissions = 0

        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æäº¤ {total_files} ä¸ªæ–‡ä»¶åˆ†æä»»åŠ¡...")

        async with aiohttp.ClientSession() as session:
            for i, file_info in enumerate(pending_files, 1):
                file_id = file_info.get("id")
                file_path = file_info.get("file_path", "unknown")

                if not file_id:
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡ä»¶: {file_path} (ç¼ºå°‘ID)")
                    failed_submissions += 1
                    continue

                # è°ƒç”¨å•æ–‡ä»¶åˆ†ææ¥å£ï¼ˆåªæäº¤ï¼Œä¸ç­‰å¾…ç»“æœï¼‰
                try:
                    url = f"{api_base_url}/api/analysis/file/{file_id}/analyze-data-model"
                    params = {"task_index": vectorstore_index, "task_id": task_id}

                    async with session.post(url, params=params) as response:
                        if response.status in [200, 202]:
                            result = await response.json()
                            if result.get("status") in ["success", "accepted"]:
                                submitted_files += 1
                                logger.info(f"âœ… [{i}/{total_files}] å·²æäº¤: {file_path}")
                            else:
                                failed_submissions += 1
                                logger.warning(f"âš ï¸ [{i}/{total_files}] æäº¤å¤±è´¥: {file_path} - {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                        else:
                            failed_submissions += 1
                            logger.warning(f"âš ï¸ [{i}/{total_files}] æäº¤å¤±è´¥: {file_path} - HTTP {response.status}")
                except Exception as e:
                    failed_submissions += 1
                    logger.warning(f"âš ï¸ [{i}/{total_files}] æäº¤å¼‚å¸¸: {file_path} - {str(e)}")

                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    try:
                        progress_callback(
                            current_file=file_path,
                            current_index=i,
                            total_files=total_files,
                            successful_files=submitted_files,
                            failed_files=failed_submissions,
                        )
                    except Exception as e:
                        logger.warning(f"Progress callback failed: {str(e)}")

        # 4. æ±‡æ€»ç»“æœ
        success_rate = (submitted_files / total_files * 100) if total_files > 0 else 0

        logger.info("ğŸ ========== æ–‡ä»¶åˆ†æä»»åŠ¡æäº¤å®Œæˆï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰==========")
        logger.info(f"ğŸ“Š æäº¤ç»Ÿè®¡:")
        logger.info(f"   - å¾…åˆ†ææ–‡ä»¶æ•°: {total_files}")
        logger.info(f"   - å·²è·³è¿‡æ–‡ä»¶æ•°: {skipped_files}")
        logger.info(f"   - æˆåŠŸæäº¤: {submitted_files}")
        logger.info(f"   - æäº¤å¤±è´¥: {failed_submissions}")
        logger.info(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"ğŸ’¡ æ³¨æ„: ä»»åŠ¡å·²æäº¤åˆ°åå°é˜Ÿåˆ—ï¼Œå°†å¼‚æ­¥æ‰§è¡Œ")

        return {
            "status": "analysis_submitted",  # æ”¹ä¸º submitted è¡¨ç¤ºå·²æäº¤ä½†æœªå®Œæˆ
            "task_id": task_id,
            "vectorstore_index": vectorstore_index,
            "total_files": total_files,
            "submitted_files": submitted_files,
            "failed_submissions": failed_submissions,
            "success_rate": f"{success_rate:.1f}%",
            "message": f"å·²æäº¤ {submitted_files}/{total_files} ä¸ªæ–‡ä»¶åˆ†æä»»åŠ¡åˆ°åå°é˜Ÿåˆ—",
        }

    except Exception as e:
        logger.error(f"æäº¤æ–‡ä»¶åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        return {"status": "failed", "task_id": task_id, "error": str(e), "message": f"æäº¤ä»»åŠ¡å¼‚å¸¸: {str(e)}"}


async def analyze_single_file_data_model(
    task_id: int, file_id: int, vectorstore_index: str, progress_callback=None
) -> Dict[str, Any]:
    """
    å•æ–‡ä»¶åˆ†ææ•°æ®æ¨¡å‹çš„ä¾¿æ·å‡½æ•°

    Args:
        task_id: ä»»åŠ¡ID
        file_id: æ–‡ä»¶ID
        vectorstore_index: å‘é‡ç´¢å¼•åç§°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    # å‡†å¤‡å…±äº«æ•°æ®
    shared = {"task_id": task_id, "file_id": file_id, "vectorstore_index": vectorstore_index, "status": "processing"}

    if progress_callback:
        shared["progress_callback"] = progress_callback

    # åˆ›å»ºå¹¶æ‰§è¡Œæµç¨‹
    flow = WebAnalysisFlow()

    try:
        # æ‰§è¡Œåˆ†ææµç¨‹
        await flow.run_async(shared)

        # è¿”å›å®Œæ•´çš„å…±äº«æ•°æ®
        return shared

    except Exception as e:
        logger.error(f"Single file data model analysis failed for file {file_id}: {str(e)}")
        shared["status"] = "failed"
        shared["error"] = str(e)
        return shared

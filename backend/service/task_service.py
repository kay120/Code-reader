import asyncio
import logging
import os
import re
import traceback
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime
import httpx

from database import SessionLocal
from models import AnalysisTask

logger = logging.getLogger(__name__)

# æ”¯æŒçš„ä»£ç æ–‡ä»¶æ‰©å±•å
SUPPORTED_CODE_EXTENSIONS = {
    # ä¸»æµç¼–ç¨‹è¯­è¨€
    ".py", ".js", ".ts", ".jsx", ".tsx", ".java", ".cpp", ".c", ".h", ".hpp",
    ".cs", ".go", ".rs", ".php", ".rb", ".swift", ".kt", ".scala", ".r",
    ".m", ".mm", ".pl", ".sh", ".bash", ".zsh", ".fish", ".ps1", ".bat", ".cmd",
    
    # é…ç½®å’Œæ ‡è®°è¯­è¨€
    ".json", ".xml", ".yaml", ".yml", ".toml", ".ini", ".cfg", ".conf",
    ".properties", ".env", ".dockerfile",
    
    # æ–‡æ¡£
    ".md", ".mdx", ".rst", ".txt", ".adoc", ".asciidoc",
    
    # Webç›¸å…³
    ".html", ".htm", ".css", ".scss", ".sass", ".less", ".vue", ".svelte",
    
    # æ•°æ®åº“
    ".sql", ".graphql", ".gql",
    
    # å…¶ä»–
    ".ipynb", ".proto", ".thrift", ".avro",
}


# è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–ç¼–ç¨‹è¯­è¨€
def get_language_from_extension(file_path: str) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–ç¼–ç¨‹è¯­è¨€ç±»å‹"""
    extension = file_path.split(".")[-1].lower() if "." in file_path else ""
    
    language_map = {
        "py": "python",
        "js": "javascript",
        "ts": "typescript",
        "tsx": "typescript",
        "jsx": "javascript",
        "java": "java",
        "cpp": "cpp",
        "c": "c",
        "cs": "csharp",
        "php": "php",
        "rb": "ruby",
        "go": "go",
        "rs": "rust",
        "kt": "kotlin",
        "swift": "swift",
        "md": "markdown",
        "txt": "text",
        "json": "json",
        "xml": "xml",
        "html": "html",
        "css": "css",
        "scss": "scss",
        "sass": "sass",
        "yml": "yaml",
        "yaml": "yaml",
        "toml": "toml",
        "ini": "ini",
        "cfg": "config",
        "conf": "config",
        "sh": "shell",
        "bat": "batch",
        "ps1": "powershell",
        "sql": "sql",
        "r": "r",
        "scala": "scala",
        "clj": "clojure",
        "hs": "haskell",
        "elm": "elm",
        "dart": "dart",
        "vue": "vue",
        "svelte": "svelte",
    }

    return language_map.get(extension, "text")


# è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡çš„æ–‡ä»¶ç±»å‹
def should_skip_file(file_path: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡è¯¥æ–‡ä»¶"""
    extension = file_path.split(".")[-1].lower() if "." in file_path else ""

    skip_extensions = {
        # å›¾ç‰‡
        "jpg", "jpeg", "png", "gif", "bmp", "svg", "ico", "webp",
        # å‹ç¼©åŒ…
        "zip", "rar", "7z", "tar", "gz", "bz2", "xz",
        # åŠå…¬æ–‡æ¡£
        "pdf", "doc", "docx", "xls", "xlsx", "ppt", "pptx",
        # åª’ä½“æ–‡ä»¶
        "mp3", "mp4", "avi", "mov", "wmv", "flv", "mkv",
        # äºŒè¿›åˆ¶æ–‡ä»¶
        "exe", "dll", "so", "dylib", "bin",
        # å­—ä½“æ–‡ä»¶
        "woff", "woff2", "ttf", "eot",
        # ä¸´æ—¶æ–‡ä»¶
        "lock", "log", "tmp", "cache",
    }
    
    return extension in skip_extensions


# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—ä»£ç è¡Œæ•°
def count_code_lines(content: str) -> int:
    """è®¡ç®—ä»£ç è¡Œæ•°ï¼ˆæ’é™¤ç©ºè¡Œï¼‰"""
    return len([line for line in content.split("\n") if line.strip()])


# è¾…åŠ©å‡½æ•°ï¼šç”ŸæˆåŸºäºæ–‡ä»¶ç±»å‹çš„æ¨¡æ‹Ÿå†…å®¹
def generate_mock_file_content(file_path: str, language: str) -> str:
    """ç”ŸæˆåŸºäºæ–‡ä»¶ç±»å‹çš„æ¨¡æ‹Ÿå†…å®¹"""
    file_name = os.path.basename(file_path)
    class_name = re.sub(r"[^a-zA-Z0-9]", "", file_name.split(".")[0])

    if language == "python":
        return f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{file_name} - Pythonæ¨¡å—
"""

import os
import sys
import re
from datetime import datetime
from typing import List, Dict, Optional

class {class_name}:
    """{file_name} ç±»"""

    def __init__(self):
        self.name = "{file_name}"
        self.created_at = datetime.now()

    def process(self, data: Dict) -> Optional[List]:
        """å¤„ç†æ•°æ®"""
        if not data:
            return None

        result = []
        for key, value in data.items():
            if isinstance(value, str):
                result.append(value.strip())

        return result

if __name__ == "__main__":
    instance = {class_name}()
    print(f"Running {{instance.name}}")'''

    elif language in ["javascript", "typescript"]:
        if language == "typescript":
            return f'''/**
 * {file_name} - TypeScriptæ¨¡å—
 */

import {{ readFile, writeFile }} from 'fs/promises';
import {{ join }} from 'path';

interface DataItem {{
    id: string;
    name: string;
    value: any;
}}

interface ProcessResult {{
    success: boolean;
    data: DataItem[];
    timestamp: Date;
}}

export class {class_name} {{
    private readonly name: string;
    private readonly version: string;

    constructor() {{
        this.name = '{file_name}';
        this.version = '1.0.0';
    }}

    public async process(items: DataItem[]): Promise<ProcessResult> {{
        if (!items || items.length === 0) {{
            throw new Error('Items array is required');
        }}

        const processedData = items.map(item => ({{
            ...item,
            processed: true,
            timestamp: new Date()
        }}));

        return {{
            success: true,
            data: processedData,
            timestamp: new Date()
        }};
    }}

    public getName(): string {{
        return this.name;
    }}
}}

export default {class_name};'''
        else:  # javascript
            return f'''/**
 * {file_name} - JavaScriptæ¨¡å—
 */

const fs = require('fs');
const path = require('path');
const util = require('util');

class {class_name} {{
    constructor() {{
        this.name = '{file_name}';
        this.version = '1.0.0';
    }}

    async process(data) {{
        if (!data) {{
            throw new Error('Data is required');
        }}

        const result = Object.keys(data).map(key => {{
            return {{
                key,
                value: data[key],
                processed: true
            }};
        }});

        return result;
    }}

    static getInstance() {{
        if (!this.instance) {{
            this.instance = new {class_name}();
        }}
        return this.instance;
    }}
}}

module.exports = {class_name};'''

    elif language == "markdown":
        return f'''# {file_name}

## æ¦‚è¿°

è¿™æ˜¯ {file_name} æ–‡æ¡£æ–‡ä»¶ã€‚

## åŠŸèƒ½ç‰¹æ€§

- åŠŸèƒ½1ï¼šæ•°æ®å¤„ç†
- åŠŸèƒ½2ï¼šæ–‡ä»¶æ“ä½œ
- åŠŸèƒ½3ï¼šé…ç½®ç®¡ç†

## ä½¿ç”¨æ–¹æ³•

```bash
# å®‰è£…ä¾èµ–
npm install

# è¿è¡Œé¡¹ç›®
npm start
```

## API æ–‡æ¡£

### æ–¹æ³•åˆ—è¡¨

| æ–¹æ³•å | å‚æ•° | è¿”å›å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| process | data: Object | Promise<Array> | å¤„ç†æ•°æ® |
| validate | input: string | boolean | éªŒè¯è¾“å…¥ |

## é…ç½®è¯´æ˜

é…ç½®æ–‡ä»¶ä½äº `config/` ç›®å½•ä¸‹ã€‚

## æ›´æ–°æ—¥å¿—

- v1.0.0: åˆå§‹ç‰ˆæœ¬
- v1.1.0: æ·»åŠ æ–°åŠŸèƒ½
'''

    elif language == "json":
        return f'''{{
  "name": "{file_name.replace('.json', '')}",
  "version": "1.0.0",
  "description": "{file_name} é…ç½®æ–‡ä»¶",
  "main": "index.js",
  "scripts": {{
    "start": "node index.js",
    "test": "jest",
    "build": "webpack --mode production"
  }},
  "dependencies": {{
    "express": "^4.18.0",
    "lodash": "^4.17.21",
    "moment": "^2.29.0"
  }},
  "devDependencies": {{
    "jest": "^28.0.0",
    "webpack": "^5.70.0"
  }},
  "keywords": [
    "config",
    "settings", 
    "application"
  ],
  "author": "Developer",
  "license": "MIT"
}}'''

    else:  # default
        return f'''# {file_name}
# è¿™æ˜¯ {file_name} æ–‡ä»¶
# æ–‡ä»¶ç±»å‹: {language}

# é…ç½®é¡¹
name = "{file_name}"
version = "1.0.0"
description = "{file_name} é…ç½®æ–‡ä»¶"

# åŸºæœ¬è®¾ç½®
debug = true
port = 3000
host = "localhost"

# æ•°æ®åº“é…ç½®
database_url = "sqlite:///app.db"
max_connections = 10

# æ—¥å¿—é…ç½®
log_level = "info"
log_file = "app.log"
'''


# è¾…åŠ©å‡½æ•°ï¼šæå–ä¾èµ–ä¿¡æ¯
def extract_dependencies(content: str, language: str) -> str:
    """æå–ä¾èµ–ä¿¡æ¯"""
    dependencies: Set[str] = set()
    
    if language == "python":
        # Python import è¯­å¥
        python_imports = re.findall(r'^(?:from\s+(\S+)\s+)?import\s+([^\n#]+)', content, re.MULTILINE)
        for from_part, import_part in python_imports:
            if from_part:
                dependencies.add(from_part.split('.')[0])
            else:
                # å¤„ç† import éƒ¨åˆ†
                parts = re.split(r'[,\s]+', import_part.strip())
                for part in parts:
                    clean_part = part.strip().split('.')[0]
                    if clean_part and not clean_part.startswith('.'):
                        dependencies.add(clean_part)
                        
    elif language in ["javascript", "typescript"]:
        # JavaScript/TypeScript import/require è¯­å¥
        js_imports = re.findall(r'(?:import.*?from\s+[\'"`]([^\'"`]+)[\'"`]|require\s*\(\s*[\'"`]([^\'"`]+)[\'"`]\s*\))', content)
        for match in js_imports:
            module_name = match[0] or match[1]
            if module_name and not module_name.startswith('.'):
                dependencies.add(module_name.split('/')[0])
                
    elif language == "java":
        # Java import è¯­å¥
        java_imports = re.findall(r'^import\s+([^;]+);', content, re.MULTILINE)
        for imp in java_imports:
            package_parts = imp.strip().split('.')
            if len(package_parts) >= 2:
                dependencies.add(f"{package_parts[0]}.{package_parts[1]}")
                
    return "|".join(sorted(dependencies))


def get_file_list_from_path(local_path: str) -> List[str]:
    """ä»æœ¬åœ°è·¯å¾„è·å–æ–‡ä»¶åˆ—è¡¨"""
    try:
        # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(local_path):
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parent.parent  # å›åˆ°backendç›®å½•
            project_root = backend_dir.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
            repo_path = project_root / local_path.lstrip('../').lstrip('..\\')
        else:
            repo_path = Path(local_path)
            
        if not repo_path.exists():
            logger.error(f"è·¯å¾„ä¸å­˜åœ¨: {local_path} -> {repo_path}")
            return []
        
        # ä½¿ç”¨ç®€åŒ–çš„æ–‡ä»¶æ‰«æé€»è¾‘
        files = []
        for file_path in repo_path.rglob("*"):
            if file_path.is_file():
                # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„å­—ç¬¦ä¸²
                relative_path = str(file_path.relative_to(repo_path))
                files.append(relative_path)
        
        logger.info(f"ä» {local_path} æ‰«æåˆ° {len(files)} ä¸ªæ–‡ä»¶")
        return files
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []


async def execute_step_0_scan_files(task_id: int, local_path: str, db) -> Dict:
    """æ­¥éª¤0: æ‰«æä»£ç æ–‡ä»¶"""
    # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
    from services import FileAnalysisService
    
    logger.info(f"å¼€å§‹æ‰§è¡Œæ­¥éª¤0: æ‰«æä»£ç æ–‡ä»¶ - ä»»åŠ¡ID: {task_id}")
    
    try:
        # è·å–æ–‡ä»¶åˆ—è¡¨
        file_list = get_file_list_from_path(local_path)
        if not file_list:
            logger.warning(f"ä»»åŠ¡ {task_id} æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            return {
                "success": True,
                "message": "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶",
                "total_files": 0,
                "successful_files": 0,
                "failed_files": 0,
                "total_code_lines": 0
            }
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_code_lines = 0
        successful_files = 0
        failed_files = 0
        
        # è·å–ä»“åº“æ ¹è·¯å¾„
        if not os.path.isabs(local_path):
            current_file = Path(__file__).resolve()
            backend_dir = current_file.parent.parent
            project_root = backend_dir.parent
            repo_path = project_root / local_path.lstrip("../")
        else:
            repo_path = Path(local_path)

        # å¯¹æ¯ä¸ªæ–‡ä»¶åˆ›å»ºåˆ†æè®°å½•
        for file_path in file_list:
            try:
                # è·³è¿‡ä¸éœ€è¦åˆ†æçš„æ–‡ä»¶ç±»å‹
                if should_skip_file(file_path):
                    logger.debug(f"è·³è¿‡æ–‡ä»¶: {file_path} (ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹)")
                    continue

                # è·å–è¯­è¨€ç±»å‹
                language = get_language_from_extension(file_path)

                # è¯»å–å®é™…æ–‡ä»¶å†…å®¹
                full_file_path = repo_path / file_path
                real_file_content = ""
                try:
                    with open(full_file_path, "r", encoding="utf-8") as f:
                        real_file_content = f.read()
                except UnicodeDecodeError:
                    # å¦‚æœUTF-8è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                    try:
                        with open(full_file_path, "r", encoding="gbk") as f:
                            real_file_content = f.read()
                    except Exception as decode_error:
                        logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {decode_error}")
                        real_file_content = ""
                except Exception as read_error:
                    logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {read_error}")
                    real_file_content = ""

                code_lines = count_code_lines(real_file_content)
                dependencies = extract_dependencies(real_file_content, language)

                # åˆ›å»ºæ–‡ä»¶åˆ†ææ•°æ®
                file_analysis_data = {
                    "task_id": task_id,
                    "file_path": file_path,
                    "language": language,
                    "analysis_version": "1.0",
                    "status": "pending",
                    "code_lines": code_lines,
                    "code_content": real_file_content,
                    "file_analysis": "",
                    "dependencies": dependencies,
                    "error_message": "",
                }

                # è°ƒç”¨æœåŠ¡åˆ›å»ºæ–‡ä»¶åˆ†æè®°å½•
                result = FileAnalysisService.create_file_analysis(file_analysis_data, db)

                if result["status"] == "success":
                    logger.debug(f"æ–‡ä»¶ {file_path} åˆ†æè®°å½•åˆ›å»ºæˆåŠŸ")
                    successful_files += 1
                    total_code_lines += code_lines
                else:
                    logger.error(f"æ–‡ä»¶ {file_path} åˆ†æè®°å½•åˆ›å»ºå¤±è´¥: {result['message']}")
                    failed_files += 1

            except Exception as file_error:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(file_error)}")
                failed_files += 1
                continue

        logger.info(f"æ­¥éª¤0å®Œæˆ - æˆåŠŸ: {successful_files}, å¤±è´¥: {failed_files}, æ€»ä»£ç è¡Œæ•°: {total_code_lines}")

        return {
            "success": True,
            "message": "æ‰«æä»£ç æ–‡ä»¶å®Œæˆ",
            "total_files": len(file_list),
            "successful_files": successful_files,
            "failed_files": failed_files,
            "total_code_lines": total_code_lines,
        }

    except Exception as e:
        logger.error(f"æ­¥éª¤0æ‰§è¡Œå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"æ­¥éª¤0æ‰§è¡Œå¤±è´¥: {str(e)}",
            "total_files": 0,
            "successful_files": 0,
            "failed_files": 0,
            "total_code_lines": 0,
        }


async def execute_step_1_create_knowledge_base(task_id: int, local_path: str, repo_info: Dict) -> Dict:
    """æ­¥éª¤1: çŸ¥è¯†åº“åˆ›å»º"""
    logger.info(f"å¼€å§‹æ‰§è¡Œæ­¥éª¤1: çŸ¥è¯†åº“åˆ›å»º - ä»»åŠ¡ID: {task_id}")

    try:
        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
        import sys
        from pathlib import Path

        current_file = Path(__file__).resolve()
        backend_dir = current_file.parent.parent
        project_root = backend_dir.parent

        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))

        # åŠ¨æ€å¯¼å…¥çŸ¥è¯†åº“åˆ›å»ºflow
        logger.info(f"è°ƒç”¨çŸ¥è¯†åº“åˆ›å»ºflow - æœ¬åœ°è·¯å¾„: {local_path}")
        from src.flows.web_flow import create_knowledge_base as create_kb_flow

        # æ‰§è¡ŒçŸ¥è¯†åº“åˆ›å»ºflow
        result = await create_kb_flow(task_id=task_id, local_path=local_path, repo_info=repo_info)

        # æ£€æŸ¥flowæ‰§è¡Œç»“æœ
        if result.get("status") == "knowledge_base_ready" and result.get("vectorstore_index"):
            logger.info(f"çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•: {result.get('vectorstore_index')}")
            return {"success": True, "message": "çŸ¥è¯†åº“åˆ›å»ºå®Œæˆ", "vectorstore_index": result.get("vectorstore_index")}
        else:
            logger.error(f"çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥: {result}")
            return {"success": False, "message": f"çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"}

    except Exception as e:
        logger.error(f"æ­¥éª¤1æ‰§è¡Œå¤±è´¥: {str(e)}")
        return {"success": False, "message": f"æ­¥éª¤1æ‰§è¡Œå¤±è´¥: {str(e)}"}


async def execute_step_2_analyze_data_model(task_id: int, vectorstore_index: str) -> Dict:
    """æ­¥éª¤2: åˆ†ææ•°æ®æ¨¡å‹"""
    logger.info(f"å¼€å§‹æ‰§è¡Œæ­¥éª¤2: åˆ†ææ•°æ®æ¨¡å‹ - ä»»åŠ¡ID: {task_id}")

    try:
        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
        import sys
        from pathlib import Path

        current_file = Path(__file__).resolve()
        backend_dir = current_file.parent.parent
        project_root = backend_dir.parent
        src_path = project_root / "src"

        if str(src_path) not in sys.path:
            sys.path.insert(0, str(src_path))

        # åŠ¨æ€å¯¼å…¥åˆ†ææ•°æ®æ¨¡å‹flow
        try:
            from src.flows.web_flow import analyze_data_model as analyze_dm_flow
        except ImportError as import_error:
            logger.error(f"æ— æ³•å¯¼å…¥åˆ†ææ•°æ®æ¨¡å‹flow: {import_error}")
            return {"success": False, "message": f"æ— æ³•å¯¼å…¥åˆ†ææ•°æ®æ¨¡å‹flow: {import_error}"}

        logger.info(f"è°ƒç”¨åˆ†ææ•°æ®æ¨¡å‹flow - å‘é‡ç´¢å¼•: {vectorstore_index}")

        # æ‰§è¡Œåˆ†ææ•°æ®æ¨¡å‹flowï¼ˆå¼‚æ­¥æ¨¡å¼ï¼šåªæäº¤ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆï¼‰
        result = await analyze_dm_flow(task_id=task_id, vectorstore_index=vectorstore_index)

        # æ£€æŸ¥flowæ‰§è¡Œç»“æœ
        if result.get("status") == "analysis_submitted":
            # å¼‚æ­¥æ¨¡å¼ï¼šä»»åŠ¡å·²æäº¤åˆ°é˜Ÿåˆ—
            total_files = result.get("total_files", 0)
            submitted_files = result.get("submitted_files", 0)
            failed_submissions = result.get("failed_submissions", 0)
            success_rate = result.get("success_rate", "0%")

            logger.info(
                f"âœ… åˆ†æä»»åŠ¡å·²æäº¤: æ€»æ–‡ä»¶ {total_files}, æˆåŠŸæäº¤ {submitted_files}, æäº¤å¤±è´¥ {failed_submissions}"
            )
            logger.info(f"ğŸ’¡ ä»»åŠ¡å°†åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹")

            return {
                "success": True,
                "message": result.get("message", "åˆ†æä»»åŠ¡å·²æäº¤åˆ°åå°é˜Ÿåˆ—"),
                "total_files": total_files,
                "submitted_files": submitted_files,
                "failed_submissions": failed_submissions,
                "success_rate": success_rate,
            }
        elif result.get("status") == "analysis_completed":
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šåŒæ­¥æ¨¡å¼
            analysis_items_count = result.get("analysis_items_count", 0)
            total_files = result.get("total_files", 0)
            successful_files = result.get("successful_files", 0)
            failed_files = result.get("failed_files", 0)
            success_rate = result.get("success_rate", "0%")

            logger.info(
                f"åˆ†ææ•°æ®æ¨¡å‹å®Œæˆ: æ€»æ–‡ä»¶ {total_files}, æˆåŠŸ {successful_files}, å¤±è´¥ {failed_files}, åˆ†æé¡¹ {analysis_items_count}"
            )

            return {
                "success": True,
                "message": result.get("message", "åˆ†ææ•°æ®æ¨¡å‹å®Œæˆ"),
                "analysis_items_count": analysis_items_count,
                "total_files": total_files,
                "successful_files": successful_files,
                "failed_files": failed_files,
                "success_rate": success_rate,
            }
        else:
            logger.error(f"åˆ†ææ•°æ®æ¨¡å‹å¤±è´¥: {result}")
            return {"success": False, "message": f"åˆ†ææ•°æ®æ¨¡å‹å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"}

    except Exception as e:
        logger.error(f"æ­¥éª¤2æ‰§è¡Œå¤±è´¥: {str(e)}")
        return {"success": False, "message": f"æ­¥éª¤2æ‰§è¡Œå¤±è´¥: {str(e)}"}


async def execute_step_3_generate_document_structure(task_id: int, external_file_path: str, repo_info: Dict = None) -> Dict:
    """æ­¥éª¤3: ç”Ÿæˆæ–‡æ¡£ç»“æ„"""
    # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
    from services import TaskReadmeService

    logger.info(f"å¼€å§‹æ‰§è¡Œæ­¥éª¤3: ç”Ÿæˆæ–‡æ¡£ç»“æ„ - ä»»åŠ¡ID: {task_id}")

    try:
        # æ£€æŸ¥external_file_pathæ˜¯å¦å·²ç»æ˜¯deepwikiçš„ä¸Šä¼ è·¯å¾„
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„,éœ€è¦å…ˆä¸Šä¼ åˆ°deepwiki
        deepwiki_path = external_file_path

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸Šä¼ : å¦‚æœè·¯å¾„åŒ…å«æœ¬åœ°ç›®å½•æ ‡è¯†,åˆ™éœ€è¦ä¸Šä¼ 
        needs_upload = (
            external_file_path.startswith("data/repos/") or
            external_file_path.startswith("./data/repos/") or
            "/data/repos/" in external_file_path
        )

        if needs_upload:
            logger.info(f"æ£€æµ‹åˆ°æœ¬åœ°è·¯å¾„,éœ€è¦ä¸Šä¼ åˆ°deepwiki: {external_file_path}")

            # å°†æœ¬åœ°è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            from pathlib import Path
            if not os.path.isabs(external_file_path):
                current_file = Path(__file__).resolve()
                backend_dir = current_file.parent.parent
                local_path = (backend_dir / external_file_path).resolve()
            else:
                local_path = Path(external_file_path)

            logger.info(f"æœ¬åœ°ç»å¯¹è·¯å¾„: {local_path}")

            # æ‰“åŒ…æˆzipå¹¶ä¸Šä¼ åˆ°deepwiki
            import tempfile
            import zipfile
            import requests

            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_zip:
                zip_path = tmp_zip.name

                # åˆ›å»ºzipæ–‡ä»¶
                logger.info(f"æ­£åœ¨æ‰“åŒ…ä»£ç ä»“åº“: {local_path}")
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(local_path):
                        # è·³è¿‡éšè—ç›®å½•å’Œå¸¸è§çš„å¿½ç•¥ç›®å½•
                        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__', 'venv', '.venv']]

                        for file in files:
                            if file.startswith('.'):
                                continue
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, local_path)
                            zipf.write(file_path, arcname)

                logger.info(f"ä»£ç ä»“åº“æ‰“åŒ…å®Œæˆ: {zip_path}")

                # ä¸Šä¼ åˆ°deepwiki
                readme_api_base_url = os.getenv("README_API_BASE_URL", "http://localhost:8001")
                upload_url = f"{readme_api_base_url}/api/upload/zip"

                logger.info(f"æ­£åœ¨ä¸Šä¼ åˆ°deepwiki: {upload_url}")

                try:
                    with open(zip_path, 'rb') as f:
                        files = {'file': (f'{local_path.name}.zip', f, 'application/x-zip-compressed')}
                        headers = {'accept': 'application/json'}
                        response = requests.post(upload_url, files=files, headers=headers, timeout=60)

                        if response.status_code == 200:
                            upload_result = response.json()
                            if upload_result.get("success"):
                                deepwiki_path = upload_result.get("file_path")
                                logger.info(f"ä¸Šä¼ æˆåŠŸ,deepwikiè·¯å¾„: {deepwiki_path}")
                            else:
                                error_msg = upload_result.get('message', 'æœªçŸ¥é”™è¯¯')
                                logger.error(f"ä¸Šä¼ å¤±è´¥: {error_msg}")
                                return {"success": False, "message": f"ä¸Šä¼ åˆ°deepwikiå¤±è´¥: {error_msg}"}
                        else:
                            logger.error(f"ä¸Šä¼ è¯·æ±‚å¤±è´¥,çŠ¶æ€ç : {response.status_code}")
                            return {"success": False, "message": f"ä¸Šä¼ åˆ°deepwikiå¤±è´¥,çŠ¶æ€ç : {response.status_code}"}
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(zip_path)
                    except:
                        pass
        else:
            logger.info(f"ä½¿ç”¨å·²æœ‰çš„deepwikiè·¯å¾„: {deepwiki_path}")

        # 1. è°ƒç”¨å¤–éƒ¨README APIç”Ÿæˆæ–‡æ¡£ç»“æ„
        logger.info("è°ƒç”¨å¤–éƒ¨README APIç”Ÿæˆæ–‡æ¡£ç»“æ„...")

        logger.info(f"ä¼ é€’ç»™README APIçš„è·¯å¾„: {deepwiki_path}")

        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®ä¸­è·å–README APIåŸºç¡€URLå’Œæ¨¡å‹é…ç½®
        readme_api_base_url = os.getenv("README_API_BASE_URL", "http://localhost:80111")
        openai_model = os.getenv("OPENAI_MODEL", "kimi-k2-0905-preview")

        # æå–é¡¹ç›®åç§°
        project_name = None
        if repo_info:
            project_name = repo_info.get("name") or repo_info.get("full_name")
            if project_name and "/" in project_name:
                # ä» "owner/repo" ä¸­æå– "repo"
                project_name = project_name.split("/")[-1]

        request_data = {
            "local_path": deepwiki_path,  # ä½¿ç”¨è½¬æ¢åçš„deepwikiè·¯å¾„
            "project_name": project_name,  # ä¼ é€’çœŸå®é¡¹ç›®åç§°
            "generate_readme": True,
            "analyze_dependencies": True,
            "generate_architecture_diagram": True,
            "language": "zh",
            "provider": "openai",
            "model": openai_model,  # ä»ç¯å¢ƒå˜é‡è¯»å–
            "export_format": "markdown",
            "analysis_depth": "detailed",
            "include_code_examples": True,
        }
        print(request_data)

        max_create_attempts = 50
        retry_delay_seconds = 5
        request_timeout = httpx.Timeout(600.0)
        readme_api_task_id = None

        async with httpx.AsyncClient(timeout=request_timeout) as client:
            for attempt in range(1, max_create_attempts + 1):
                try:
                    response = await client.post(f"{readme_api_base_url}/api/analyze/local", json=request_data)
                except httpx.RequestError as exc:
                    logger.warning(f"æ–‡æ¡£ç»“æ„ç”Ÿæˆä»»åŠ¡åˆ›å»ºè¯·æ±‚å¤±è´¥(ç¬¬ {attempt}/{max_create_attempts} æ¬¡å°è¯•): {exc}")
                    if attempt < max_create_attempts:
                        await asyncio.sleep(retry_delay_seconds)
                    continue

                if response.status_code == 200:
                    result = response.json()
                    print(result)
                    readme_api_task_id = result.get("task_id")
                    logger.info(f"æ–‡æ¡£ç»“æ„ç”Ÿæˆä»»åŠ¡å·²åˆ›å»ºï¼Œä»»åŠ¡ID: {readme_api_task_id}")
                    break

                error_data = (
                    response.json() if response.headers.get("content-type", "").startswith("application/json") else {}
                )
                logger.error(f"æ–‡æ¡£ç»“æ„ç”Ÿæˆä»»åŠ¡åˆ›å»ºå¤±è´¥(ç¬¬ {attempt}/{max_create_attempts} æ¬¡å°è¯•): {error_data}")
                if attempt < max_create_attempts:
                    logger.info(f"å°†åœ¨ {retry_delay_seconds} ç§’åé‡è¯•åˆ›å»ºæ–‡æ¡£ç»“æ„ä»»åŠ¡")
                    await asyncio.sleep(retry_delay_seconds)
                else:
                    return {
                        "success": False,
                        "message": f"æ–‡æ¡£ç»“æ„ç”Ÿæˆä»»åŠ¡åˆ›å»ºå¤±è´¥: {error_data.get('message', response.text)}",
                    }

        if not readme_api_task_id:
            return {"success": False, "message": "æ–‡æ¡£ç»“æ„ç”Ÿæˆä»»åŠ¡åˆ›å»ºå¤±è´¥: è¯·æ±‚åœ¨æœ€å¤§é‡è¯•æ¬¡æ•°åä»æœªæˆåŠŸ"}

        # 2. è½®è¯¢æ£€æŸ¥ç”ŸæˆçŠ¶æ€
        logger.info("å¼€å§‹è½®è¯¢æ£€æŸ¥æ–‡æ¡£ç”ŸæˆçŠ¶æ€...")
        completed = False
        attempts = 0
        max_attempts = 60  # æœ€å¤šè½®è¯¢60æ¬¡ï¼ˆ5åˆ†é’Ÿï¼‰
        poll_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡

        while not completed and attempts < max_attempts:
            attempts += 1
            logger.info(f"ç¬¬ {attempts} æ¬¡æ£€æŸ¥æ–‡æ¡£ç”ŸæˆçŠ¶æ€...")

            try:
                # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’ï¼Œå¹¶æ·»åŠ é‡è¯•é€»è¾‘
                async with httpx.AsyncClient(timeout=60.0) as client:
                    status_response = await client.get(
                        f"{readme_api_base_url}/api/analyze/local/{readme_api_task_id}/status"
                    )

                    if status_response.status_code == 200:
                        status_result = status_response.json()

                        if status_result.get("status") == "completed":
                            logger.info("æ–‡æ¡£ç»“æ„ç”Ÿæˆå®Œæˆ!")

                            # 3. è·å–ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹
                            if status_result.get("result") and status_result["result"].get("markdown"):
                                markdown_content = status_result["result"]["markdown"]
                                logger.info(f"è·å–åˆ°ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(markdown_content)}")

                                # 4. ä¿å­˜åˆ°æœ¬é¡¹ç›®æ•°æ®åº“
                                logger.info("ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“...")
                                readme_data = {"task_id": task_id, "content": markdown_content}

                                # ä½¿ç”¨æ•°æ®åº“ä¼šè¯æ¥ä¿å­˜æ–‡æ¡£

                                with SessionLocal() as db:
                                    save_result = TaskReadmeService.create_task_readme(readme_data, db)

                                    if save_result["status"] == "success":
                                        logger.info("æ–‡æ¡£ç»“æ„ç”Ÿæˆå¹¶ä¿å­˜æˆåŠŸ")
                                        completed = True
                                        return {
                                            "success": True,
                                            "message": "æ–‡æ¡£ç»“æ„ç”Ÿæˆå¹¶ä¿å­˜æˆåŠŸ",
                                            "content_length": len(markdown_content),
                                        }
                                    else:
                                        logger.error(f"ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥: {save_result['message']}")
                                        return {
                                            "success": False,
                                            "message": f"ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“å¤±è´¥: {save_result['message']}",
                                        }
                            else:
                                logger.error("ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹ä¸ºç©º")
                                return {"success": False, "message": "ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹ä¸ºç©º"}

                        elif status_result.get("status") == "failed" or status_result.get("error"):
                            logger.error(f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {status_result.get('error', status_result.get('message'))}")
                            return {
                                "success": False,
                                "message": f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {status_result.get('error', status_result.get('message'))}",
                            }
                        else:
                            # ä»åœ¨è¿›è¡Œä¸­ï¼Œæ˜¾ç¤ºè¿›åº¦
                            progress = status_result.get("progress", 0)
                            current_stage = status_result.get("current_stage", "å¤„ç†ä¸­")
                            logger.info(f"æ–‡æ¡£ç”Ÿæˆè¿›åº¦: {progress}% - {current_stage}")

                            # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                            await asyncio.sleep(poll_interval)
                    else:
                        logger.error(f"æ£€æŸ¥æ–‡æ¡£ç”ŸæˆçŠ¶æ€å¤±è´¥: HTTP {status_response.status_code}")
                        await asyncio.sleep(poll_interval)

            except (httpx.ReadTimeout, httpx.ConnectTimeout, httpx.TimeoutException) as timeout_error:
                logger.warning(f"ç¬¬ {attempts} æ¬¡çŠ¶æ€æ£€æŸ¥è¶…æ—¶: {str(timeout_error)}")
                print(f"çŠ¶æ€æ£€æŸ¥è¶…æ—¶ (ç¬¬{attempts}æ¬¡): {str(timeout_error)}")

                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­ç­‰å¾…åé‡è¯•
                if attempts < max_attempts:
                    logger.info(f"ç­‰å¾… {poll_interval} ç§’åé‡è¯•...")
                    await asyncio.sleep(poll_interval)
                else:
                    logger.error("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒçŠ¶æ€æ£€æŸ¥å¤±è´¥")

            except httpx.RequestError as req_error:
                logger.error(f"ç¬¬ {attempts} æ¬¡çŠ¶æ€æ£€æŸ¥è¯·æ±‚é”™è¯¯: {str(req_error)}")
                print(f"çŠ¶æ€æ£€æŸ¥è¯·æ±‚é”™è¯¯ (ç¬¬{attempts}æ¬¡): {str(req_error)}")

                # ç½‘ç»œé”™è¯¯ä¹Ÿç»§ç»­é‡è¯•
                if attempts < max_attempts:
                    logger.info(f"ç­‰å¾… {poll_interval} ç§’åé‡è¯•...")
                    await asyncio.sleep(poll_interval)
                else:
                    logger.error("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒçŠ¶æ€æ£€æŸ¥å¤±è´¥")

            except Exception as unexpected_error:
                logger.error(f"çŠ¶æ€æ£€æŸ¥å‘ç”Ÿæ„å¤–é”™è¯¯: {str(unexpected_error)}")
                print(f"çŠ¶æ€æ£€æŸ¥æ„å¤–é”™è¯¯: {str(unexpected_error)}")
                # æ„å¤–é”™è¯¯ä¹Ÿé‡è¯•
                await asyncio.sleep(poll_interval)

        if not completed:
            logger.error("æ–‡æ¡£ç”Ÿæˆè¶…æ—¶")
            return {"success": False, "message": "æ–‡æ¡£ç”Ÿæˆè¶…æ—¶"}

    except Exception as e:
        error_traceback = traceback.format_exc()
        logger.error(f"æ­¥éª¤3æ‰§è¡Œå¤±è´¥: {str(e)}")
        logger.error(f"æ­¥éª¤3å¼‚å¸¸è¯¦ç»†ä¿¡æ¯:\n{error_traceback}")
        print(f"=== æ­¥éª¤3å¼‚å¸¸è¯¦ç»†ä¿¡æ¯ ===\n{error_traceback}")
        return {"success": False, "message": f"æ­¥éª¤3æ‰§è¡Œå¤±è´¥: {str(e)}"}


async def run_task(task_id: int, external_file_path: str):
    """è¿è¡Œåˆ†æä»»åŠ¡çš„ä¸»å‡½æ•°"""
    # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
    from services import AnalysisTaskService, RepositoryService
    from database import get_db_async

    logger.info(f"å¼€å§‹è¿è¡Œä»»åŠ¡ {task_id}")

    async with get_db_async() as db:
        try:
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task_result = AnalysisTaskService.get_task_by_id(task_id, db)
            if task_result["status"] != "success":
                logger.error(f"è·å–ä»»åŠ¡å¤±è´¥: {task_result['message']}")
                return {"status": "error", "message": task_result["message"]}

            task_data = task_result["task"]
            repository_id = task_data["repository_id"]

            # è·å–ä»“åº“ä¿¡æ¯
            repo_result = RepositoryService.get_repository_by_id(repository_id, db)
            if repo_result["status"] != "success":
                logger.error(f"è·å–ä»“åº“å¤±è´¥: {repo_result['message']}")
                return {"status": "error", "message": repo_result["message"]}

            repository = repo_result["repository"]
            local_path = repository.get("local_path")

            if not local_path:
                logger.error(f"ä»“åº“ {repository_id} æ²¡æœ‰æœ¬åœ°è·¯å¾„")
                return {"status": "error", "message": "ä»“åº“ç¼ºå°‘æœ¬åœ°è·¯å¾„"}

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            task_obj = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
            if task_obj:
                task_obj.status = "running"
                if not task_obj.start_time:  # åªåœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶è®¾ç½®å¼€å§‹æ—¶é—´
                    task_obj.start_time = datetime.now()
                db.commit()

            logger.info(f"ä»»åŠ¡ {task_id} ä½¿ç”¨ä»“åº“è·¯å¾„: {local_path}")

            # å‡†å¤‡ä»“åº“ä¿¡æ¯
            repo_info = {
                "full_name": repository.get("full_name") or repository.get("name"),
                "name": repository.get("name"),
                "local_path": local_path,
            }

            # ========== æ£€æŸ¥æ˜¯å¦æ˜¯æ¢å¤ä»»åŠ¡ ==========
            is_resume = False
            vectorstore_index = None

            if task_obj and task_obj.task_index:
                # å¦‚æœå·²æœ‰ task_indexï¼Œè¯´æ˜æ­¥éª¤0å’Œæ­¥éª¤1å·²å®Œæˆ
                is_resume = True
                vectorstore_index = task_obj.task_index
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¢å¤ä»»åŠ¡ï¼Œè·³è¿‡æ­¥éª¤0å’Œæ­¥éª¤1ï¼Œä½¿ç”¨å·²æœ‰ç´¢å¼•: {vectorstore_index}")
                logger.info(f"ğŸ“Š å½“å‰è¿›åº¦: {task_obj.successful_files}/{task_obj.total_files} ä¸ªæ–‡ä»¶å·²å®Œæˆ")

            # ========== æ‰§è¡Œ4ä¸ªåˆ†ææ­¥éª¤ ==========

            if not is_resume:
                # æ­¥éª¤0: æ‰«æä»£ç æ–‡ä»¶
                logger.info("=== å¼€å§‹æ‰§è¡Œæ­¥éª¤0: æ‰«æä»£ç æ–‡ä»¶ ===")
                step0_result = await execute_step_0_scan_files(task_id, local_path, db)

                if not step0_result["success"]:
                    logger.error(f"æ­¥éª¤0å¤±è´¥: {step0_result['message']}")
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                    if task_obj:
                        task_obj.status = "failed"
                        task_obj.end_time = datetime.now()
                        db.commit()
                    return {"status": "error", "message": f"æ­¥éª¤0å¤±è´¥: {step0_result['message']}"}

                # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯
                if task_obj:
                    task_obj.total_files = step0_result.get("total_files", 0)
                    # æ³¨æ„ï¼šæ­¥éª¤ 0 åªæ˜¯æ‰«ææ–‡ä»¶ï¼Œä¸æ›´æ–° successful_files
                    # successful_files åº”è¯¥åœ¨æ–‡ä»¶å®é™…åˆ†æå®Œæˆæ—¶æ›´æ–°
                    task_obj.successful_files = 0  # åˆå§‹åŒ–ä¸º 0
                    task_obj.failed_files = 0  # åˆå§‹åŒ–ä¸º 0
                    task_obj.code_lines = step0_result.get("total_code_lines", 0)
                    db.commit()

                logger.info(f"æ­¥éª¤0å®Œæˆ: {step0_result['message']}")

                # æ­¥éª¤1: çŸ¥è¯†åº“åˆ›å»º
                logger.info("=== å¼€å§‹æ‰§è¡Œæ­¥éª¤1: çŸ¥è¯†åº“åˆ›å»º ===")
                step1_result = await execute_step_1_create_knowledge_base(task_id, local_path, repo_info)

                if not step1_result["success"]:
                    logger.error(f"æ­¥éª¤1å¤±è´¥: {step1_result['message']}")
                    if task_obj:
                        task_obj.status = "failed"
                        task_obj.end_time = datetime.now()
                        db.commit()
                    return {"status": "error", "message": f"æ­¥éª¤1å¤±è´¥: {step1_result['message']}"}

                vectorstore_index = step1_result.get("vectorstore_index")

                # æ›´æ–°ä»»åŠ¡ç´¢å¼•
                if task_obj and vectorstore_index:
                    task_obj.task_index = vectorstore_index
                    db.commit()

                logger.info(f"æ­¥éª¤1å®Œæˆ: {step1_result['message']}")
            else:
                # æ¢å¤ä»»åŠ¡ï¼Œä½¿ç”¨å·²æœ‰çš„ç»Ÿè®¡ä¿¡æ¯
                step0_result = {
                    "success": True,
                    "message": "è·³è¿‡æ­¥éª¤0ï¼ˆæ¢å¤ä»»åŠ¡ï¼‰",
                    "total_files": task_obj.total_files,
                    "total_code_lines": task_obj.code_lines,
                }
                step1_result = {
                    "success": True,
                    "message": "è·³è¿‡æ­¥éª¤1ï¼ˆæ¢å¤ä»»åŠ¡ï¼‰",
                    "vectorstore_index": vectorstore_index,
                }

            # æ­¥éª¤2: åˆ†ææ•°æ®æ¨¡å‹
            logger.info("=== å¼€å§‹æ‰§è¡Œæ­¥éª¤2: åˆ†ææ•°æ®æ¨¡å‹ ===")

            if not vectorstore_index:
                logger.error("ç¼ºå°‘å‘é‡ç´¢å¼•ï¼Œæ— æ³•æ‰§è¡Œæ•°æ®æ¨¡å‹åˆ†æ")
                if task_obj:
                    task_obj.status = "failed"
                    task_obj.end_time = datetime.now()
                    db.commit()
                return {"status": "error", "message": "ç¼ºå°‘å‘é‡ç´¢å¼•ï¼Œæ— æ³•æ‰§è¡Œæ•°æ®æ¨¡å‹åˆ†æ"}

            step2_result = await execute_step_2_analyze_data_model(task_id, vectorstore_index)

            if not step2_result["success"]:
                logger.error(f"æ­¥éª¤2å¤±è´¥: {step2_result['message']}")
                if task_obj:
                    task_obj.status = "failed"
                    task_obj.end_time = datetime.now()
                    db.commit()
                return {"status": "error", "message": f"æ­¥éª¤2å¤±è´¥: {step2_result['message']}"}

            logger.info(f"æ­¥éª¤2å®Œæˆ: {step2_result['message']}")

            # ========== å¼‚æ­¥æ¨¡å¼ï¼šæ­¥éª¤ 2 å®Œæˆåç«‹å³ç»“æŸï¼Œé‡Šæ”¾ Worker ==========
            # æ­¥éª¤ 3 å°†åœ¨æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆåè‡ªåŠ¨è§¦å‘

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º processingï¼ˆè¡¨ç¤ºæ­£åœ¨åå°å¤„ç†ï¼‰
            if task_obj:
                task_obj.status = "processing"  # æ”¹ä¸º processingï¼Œè¡¨ç¤ºæ–‡ä»¶åˆ†ææ­£åœ¨åå°æ‰§è¡Œ
                task_obj.progress_percentage = 50  # æ­¥éª¤ 0-1 å®Œæˆï¼Œæ­¥éª¤ 2 å·²æäº¤
                db.commit()

            logger.info(f"âœ… ä»»åŠ¡ {task_id} æ­¥éª¤ 0-2 å®Œæˆï¼Œæ–‡ä»¶åˆ†æä»»åŠ¡å·²æäº¤åˆ°åå°é˜Ÿåˆ—")
            logger.info(f"ğŸ’¡ æ­¥éª¤ 3ï¼ˆç”Ÿæˆæ–‡æ¡£ï¼‰å°†åœ¨æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆåè‡ªåŠ¨è§¦å‘")

            return {
                "status": "success",
                "message": "ä»»åŠ¡å·²æäº¤åˆ°åå°é˜Ÿåˆ—ï¼Œæ­£åœ¨å¼‚æ­¥æ‰§è¡Œ",
                "task_id": task_id,
                "step0_result": step0_result,
                "step1_result": step1_result,
                "step2_result": step2_result,
            }

        except Exception as e:
            logger.error(f"è¿è¡Œä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            try:
                task_obj = db.query(AnalysisTask).filter(AnalysisTask.id == task_id).first()
                if task_obj:
                    task_obj.status = "failed"
                    task_obj.end_time = datetime.now()
                    task_obj.current_file = None  # æ¸…ç©ºå½“å‰å¤„ç†æ–‡ä»¶
                    db.commit()
            except Exception as update_error:
                logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(update_error)}")

            return {"status": "error", "message": f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}"}
